@book{burnhamModel2002,
  title = {Model Selection and Multimodel Inference: A Practical Information-theoretic Approach},
  shorttitle = {Model {{Selection}} and {{Multimodel Inference}}},
  author = {Burnham, Kenneth P. and Anderson, David R.},
  year = {2002},
  publisher = {{Springer}},
  abstract = {The second edition of this book is unique in that it focuses on methods for making formal statistical inference from all the models in an a priori set (Multi-Model Inference). A philosophy is presented for model-based data analysis and a general strategy outlined for the analysis of empirical data. The book invites increased attention on a priori science hypotheses and modeling.Kullback-Leibler Information represents a fundamental quantity in science and is Hirotugu Akaike's basis for model selection. The maximized log-likelihood function can be bias-corrected as an estimator of expected, relative Kullback-Leibler information. This leads to Akaike's Information Criterion (AIC) and various extensions. These methods are relatively simple and easy to use in practice, but based on deep statistical theory. The information theoretic approaches provide a unified and rigorous theory, an extension of likelihood theory, an important application of information theory, and are objective and practical to employ across a very wide class of empirical problems.The book presents several new ways to incorporate model selection uncertainty into parameter estimates and estimates of precision. An array of challenging examples is given to illustrate various technical issues.This is an applied book written primarily for biologists and statisticians wanting to make inferences from multiple models and is suitable as a graduate text or as a reference for professional analysts.},
  isbn = {978-0-387-95364-9},
  langid = {english},
  keywords = {biology,Biology - Mathematical models,Biology/ Mathematical models,Mathematical statistics,Mathematics / General,Mathematics / Probability \& Statistics / General,Medical / Biostatistics,Science / General,Science / Life Sciences / Biology}
}

@book{bolkerEcological2008a,
  title = {Ecological Models and Data in {R}},
  author = {Bolker, Benjamin M.},
  year = {2008},
  month = jul,
  publisher = {{Princeton University Press}},
  isbn = {0-691-12522-8},
  annote = {PDF available on request ... Chapter 6 presents the basics of likelihood and Bayesian model-fitting approaches. Chapter 7 covers the basics of MCMC sampling}
}


@article{makowskiIndices2019,
  title = {Indices of Effect Existence and Significance in the {Bayesian} Framework},
  author = {Makowski, Dominique and {Ben-Shachar}, Mattan S. and Chen, S. H. Annabel and L{\"u}decke, Daniel},
  year = {2019},
  journal = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  urldate = {2022-07-21},
  abstract = {Turmoil has engulfed psychological science. Causes and consequences of the reproducibility crisis are in dispute. With the hope of addressing some of its aspects, Bayesian methods are gaining increasing attention in psychological science. Some of their advantages, as opposed to the frequentist framework, are the ability to describe parameters in probabilistic terms and explicitly incorporate prior knowledge about them into the model. These issues are crucial in particular regarding the current debate about statistical significance. Bayesian methods are not necessarily the only remedy against incorrect interpretations or wrong conclusions, but there is an increasing agreement that they are one of the keys to avoid such fallacies. Nevertheless, its flexible nature is its power and weakness, for there is no agreement about what indices of ``significance'' should be computed or reported. This lack of a consensual index or guidelines, such as the frequentist p-value, further contributes to the unnecessary opacity that many non-familiar readers perceive in Bayesian statistics. Thus, this study describes and compares several Bayesian indices, provide intuitive visual representation of their ``behavior'' in relationship with common sources of variance such as sample size, magnitude of effects and also frequentist significance. The results contribute to the development of an intuitive understanding of the values that researchers report, allowing to draw sensible recommendations for Bayesian statistics description, critical for the standardization of scientific reporting.},
  file = {/home/bolker/Zotero/storage/2T3CMMW9/Makowski et al. - 2019 - Indices of Effect Existence and Significance in th.pdf}
}

@book{mccarthyBayesian2007,
  title = {Bayesian Methods for Ecology},
  author = {McCarthy, M.},
  year = {2007},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, England}},
  annote = {Good, ecologist-friendly introduction; focus on build-your-own-models with BUGS. Illustrates GLM-like models, mark-recapture analysis. Nice examples of conservation biology problems with informative priors (see also McCarthy and Masters 2005).}
}

@article{mccarthyClarifying2004,
  title = {Clarifying the Effect of Toe Clipping on Frogs with {{Bayesian}} Statistics},
  author = {McCarthy, Michael A. and Parris, Kirsten M.},
  year = {2004},
  journal = {Journal of Applied Ecology},
  volume = {41},
  pages = {780--786}
}

@article{mccarthyProfiting2005,
  title = {Profiting from Prior Information in {Bayesian} Analyses of Ecological Data},
  author = {McCarthy, Michael A. and Masters, Pip},
  year = {2005},
  journal = {Journal of Applied Ecology},
  volume = {42},
  number = {6},
  pages = {1012--1019}
}

@book{mcelreathStatistical2020,
  title = {Statistical Rethinking: A {Bayesian} Course with Examples in {R} and {Stan}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  publisher = {{CRC Press}},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  googlebooks = {6H\_WDwAAQBAJ},
  isbn = {978-0-429-63914-2},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General}
}

@book{clarkModels2020,
  title = {Models for {{Ecological Data}}: {{An Introduction}}},
  shorttitle = {Models for {{Ecological Data}}},
  author = {Clark, James S.},
  year = {2020},
  month = oct,
  publisher = {{Princeton University Press}},
  abstract = {The environmental sciences are undergoing a revolution in the use of models and data. Facing ecological data sets of unprecedented size and complexity, environmental scientists are struggling to understand and exploit powerful new statistical tools for making sense of ecological processes.In Models for Ecological Data, James Clark introduces ecologists to these modern methods in modeling and computation. Assuming only basic courses in calculus and statistics, the text introduces readers to basic maximum likelihood and then works up to more advanced topics in Bayesian modeling and computation. Clark covers both classical statistical approaches and powerful new computational tools and describes how complexity can motivate a shift from classical to Bayesian methods. Through an available lab manual, the book introduces readers to the practical work of data modeling and computation in the language R. Based on a successful course at Duke University and National Science Foundation-funded institutes on hierarchical modeling, Models for Ecological Data will enable ecologists and other environmental scientists to develop useful models that make sense of ecological data.Consistent treatment from classical to modern BayesUnderlying distribution theory to algorithm developmentMany examples and applicationsDoes not assume statistical backgroundExtensive supporting appendixesLab manual in R is available separately},
  langid = {english}
}

@book{hobbsBayesian2015,
  title = {Bayesian {{Models}}: {{A Statistical Primer}} for {{Ecologists}}},
  shorttitle = {Bayesian {{Models}}},
  author = {Hobbs, N. Thompson and Hooten, Mevin B.},
  year = {2015},
  month = aug,
  publisher = {{Princeton University Press}},
  address = {{Princeton, New Jersey}},
  abstract = {Bayesian modeling has become an indispensable tool for ecological research because it is uniquely suited to deal with complexity in a statistically coherent way. This textbook provides a comprehensive and accessible introduction to the latest Bayesian methods\textemdash in language ecologists can understand. Unlike other books on the subject, this one emphasizes the principles behind the computations, giving ecologists a big-picture understanding of how to implement this powerful statistical approach.Bayesian Models is an essential primer for non-statisticians. It begins with a definition of probability and develops a step-by-step sequence of connected ideas, including basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and inference from single and multiple models. This unique book places less emphasis on computer coding, favoring instead a concise presentation of the mathematical statistics needed to understand how and why Bayesian analysis works. It also explains how to write out properly formulated hierarchical Bayesian models and use them in computing, research papers, and proposals.This primer enables ecologists to understand the statistical principles behind Bayesian modeling and apply them to research, teaching, policy, and management.Presents the mathematical and statistical foundations of Bayesian modeling in language accessible to non-statisticiansCovers basic distribution theory, network diagrams, hierarchical models, Markov chain Monte Carlo, and moreDeemphasizes computer coding in favor of basic principlesExplains how to write out properly factored statistical expressions representing Bayesian models},
  isbn = {978-0-691-15928-7},
  langid = {english}
}

@article{elderdUncertainty2006a,
  title = {Uncertainty in Predictions of Disease Spread and Public Health Responses to Bioterrorism and Emerging Diseases},
  author = {Elderd, Bret D. and Dukic, Vanja M. and Dwyer, Greg},
  year = {2006},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {103},
  number = {42},
  pages = {15693--15697},
  doi = {10.1073/pnas.0600816103},
  urldate = {2008-06-01},
  abstract = {Concerns over bioterrorism and emerging diseases have led to the widespread use of epidemic models for evaluating public health strategies. Partly because epidemic models often capture the dynamics of prior epidemics remarkably well, little attention has been paid to how uncertainty in parameter estimates might affect model predictions. To understand such effects, we used Bayesian statistics to rigorously estimate the uncertainty in the parameters of an epidemic model, focusing on smallpox bioterrorism. We then used a vaccination model to translate the uncertainty in the model parameters into uncertainty in which of two vaccination strategies would provide a better response to bioterrorism, mass vaccination, or vaccination of social contacts, so-called "trace vaccination." Our results show that the uncertainty in the model parameters is remarkably high and that this uncertainty has important implications for vaccination strategies. For example, under one plausible scenario, the most likely outcome is that mass vaccination would save \{approx\}100,000 more lives than trace vaccination. Because of the high uncertainty in the parameters, however, there is also a substantial probability that mass vaccination would save 200,000 or more lives than trace vaccination. In addition to providing the best response to the most likely outcome, mass vaccination thus has the advantage of preventing outcomes that are only slightly less likely but that are substantially more horrific. Rigorous estimates of uncertainty thus can reveal hidden advantages of public health strategies, suggesting that formal uncertainty estimation should play a key role in planning for epidemics.},
  file = {/home/bolker/Zotero/storage/J39ME5MK/Elderd et al - 2006 - Uncertainty in predictions of disease spread and p.pdf;/home/bolker/Zotero/storage/ZTAIUQI9/15693.html}
}

@article{ludwigUncertainty1996,
  title = {Uncertainty and the {{Assessment}} of {{Extinction Probabilities}}},
  author = {Ludwig, Donald},
  year = {1996},
  journal = {Ecological Applications},
  volume = {6},
  number = {4},
  pages = {1067--1076},
  issn = {1939-5582},
  doi = {10.2307/2269591},
  urldate = {2021-03-31},
  abstract = {A proper assessment of the probability of early collapse or extinction of a population requires consideration of our uncertainty about crucial parameters and processes. Simple simulation approaches to assessment consider only a single set of parameter values, but what is required is consideration of all more or less plausible combinations of parameters. Bayesian decision theory is an appropriate tool for such assessment. I contrast standard (frequentist) and Bayesian approaches to a simple regression problem. I use these results to calculate the probability of early population collapse for three data sets relating to the Palila, Laysan Finch, and Snow Goose. The Bayesian results imply much higher risk of early collapse than maximum likelihood methods. This difference is due to large probabilities of early collapse for certain parameter values that are plausible in light of the data. Because of simplifying assumptions, these results are not directly applicable to assessment. Nevertheless they imply that maximum likelihood and similar methods based upon point parameter estimates will grossly underestimate the risk of early collapse.},
  copyright = {\textcopyright{} 1996 by the Ecological Society of America},
  langid = {english},
  file = {/home/bolker/Zotero/storage/6LY7PVRC/2269591.html}
}

@article{bannerUse2020,
  title = {The Use of {{Bayesian}} Priors in {{Ecology}}: {{The}} Good, the Bad and the Not Great},
  shorttitle = {The Use of {{Bayesian}} Priors in {{Ecology}}},
  author = {Banner, Katharine M. and Irvine, Kathryn M. and Rodhouse, Thomas J.},
  year = {2020},
  journal = {Methods in Ecology and Evolution},
  volume = {11},
  number = {8},
  pages = {882--889},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13407},
  urldate = {2023-05-19},
  abstract = {Bayesian data analysis (BDA) is a powerful tool for making inference from ecological data, but its full potential has yet to be realized. Despite a generally positive trajectory in research surrounding model development and assessment, far too little attention has been given to prior specification. Default priors, a sub-class of non-informative prior distributions that are often chosen without critical thought or evaluation, are commonly used in practice. We believe the fear of being too `subjective' has prevented many researchers from using any prior information in their analyses despite the fact that defending prior choice (informative or not) promotes good statistical practice. In this commentary, we provide an overview of how BDA is currently being used in a random sample of articles, discuss implications for inference if current bad practices continue, and highlight sub-fields where knowledge about the system has improved inference and promoted good statistical practices through the careful and justified use of informative priors. We hope to inspire a renewed discussion about the use of Bayesian priors in Ecology with particular attention paid to specification and justification. We also emphasize that all priors are the result of a subjective choice, and should be discussed in that way.},
  langid = {english},
  keywords = {Bayesian hierarchical models,good statistical practice,sensitivity analysis,subjective priors},
  file = {/home/bolker/Zotero/storage/5ZGNPEXS/Banner et al. - 2020 - The use of Bayesian priors in Ecology The good, t.pdf;/home/bolker/Zotero/storage/67KI3V63/2041-210X.html}
}

@article{lemoineMoving2019,
  title = {Moving beyond Noninformative Priors: Why and How to Choose Weakly Informative Priors in {{Bayesian}} Analyses},
  shorttitle = {Moving beyond Noninformative Priors},
  author = {Lemoine, Nathan P.},
  year = {2019},
  journal = {Oikos},
  volume = {128},
  number = {7},
  pages = {912--928},
  issn = {1600-0706},
  doi = {10.1111/oik.05985},
  urldate = {2021-06-11},
  abstract = {Throughout the last two decades, Bayesian statistical methods have proliferated throughout ecology and evolution. Numerous previous references established both philosophical and computational guidelines for implementing Bayesian methods. However, protocols for incorporating prior information, the defining characteristic of Bayesian philosophy, are nearly nonexistent in the ecological literature. Here, I hope to encourage the use of weakly informative priors in ecology and evolution by providing a `consumer's guide' to weakly informative priors. The first section outlines three reasons why ecologists should abandon noninformative priors: 1) common flat priors are not always noninformative, 2) noninformative priors provide the same result as simpler frequentist methods, and 3) noninformative priors suffer from the same high type I and type M error rates as frequentist methods. The second section provides a guide for implementing informative priors, wherein I detail convenient `reference' prior distributions for common statistical models (i.e. regression, ANOVA, hierarchical models). I then use simulations to visually demonstrate how informative priors influence posterior parameter estimates. With the guidelines provided here, I hope to encourage the use of weakly informative priors for Bayesian analyses in ecology. Ecologists can and should debate the appropriate form of prior information, but should consider weakly informative priors as the new `default' prior for any Bayesian model.},
  copyright = {\textcopyright{} 2019 The Authors},
  langid = {english},
  keywords = {Bayesian statistics,frequentist statistics,Markov chain Monte Carlo,vague priors},
  file = {/home/bolker/Zotero/storage/C3PL4BG6/Lemoine - 2019 - Moving beyond noninformative priors why and how t.pdf;/home/bolker/Zotero/storage/RHUN3DEC/oik.html}
}


@article{cromeNovel1996,
	title = {A {Novel} {Bayesian} {Approach} to {Assessing} {Impacts} of {Rain} {Forest} {Logging}},
	volume = {6},
	journal = {Ecological Applications},
	author = {Crome, F. H. J. and Thomas, M. R. and Moore, L. A.},
	year = {1996},
	pages = {1104--1123},
}


@article{edwardsComment1996,
	title = {Comment: {The} {First} {Data} {Analysis} {Should} be {Journalistic}},
	volume = {6},
	number = {4},
	journal = {Ecological Applications},
	author = {Edwards, Don},
	year = {1996},
	pages = {1090--1094},
}


@book{nicenboimIntroduction2023,
	title = {An Introduction to {Bayesian} Data Analysis for Cognitive Science},
	url = {https://vasishth.github.io/bayescogsci/book/},
	author = {Nicenboim, Bruno and Schad, Daniel and Vasishth, Shravan}
}


@article{gelmanBayesian2020,
	title = {Bayesian {Workflow}},
	url = {http://arxiv.org/abs/2011.01808},
	abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
	urldate = {2020-11-04},
	journal = {arXiv:2011.01808 [stat]},
	author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.01808},
	keywords = {Statistics - Methodology},
	annote = {Comment: 77 pages, 35 figures},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/HYG8WGKB/Gelman et al. - 2020 - Bayesian Workflow.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/MK6YCMEY/2011.html:text/html},
}

@article{taltsValidating2020,
	title = {Validating {Bayesian} {Inference} {Algorithms} with {Simulation}-{Based} {Calibration}},
	url = {http://arxiv.org/abs/1804.06788},
	abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce {\textbackslash}emph\{simulation-based calibration\} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
	urldate = {2021-12-02},
	journal = {arXiv:1804.06788 [stat]},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv: 1804.06788},
	keywords = {Statistics - Methodology},
	annote = {Comment: 19 pages, 13 figures},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/AK47EJTW/Talts et al. - 2020 - Validating Bayesian Inference Algorithms with Simu.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/YAUKHIW4/1804.html:text/html},
}

@article{grinsztajnBayesian2021,
	title = {Bayesian workflow for disease transmission modeling in {Stan}},
	volume = {40},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9164},
	doi = {10.1002/sim.9164},
	abstract = {This tutorial shows how to build, fit, and criticize disease transmission models in Stan, and should be useful to researchers interested in modeling the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic and other infectious diseases in a Bayesian framework. Bayesian modeling provides a principled way to quantify uncertainty and incorporate both data and prior knowledge into the model estimates. Stan is an expressive probabilistic programming language that abstracts the inference and allows users to focus on the modeling. As a result, Stan code is readable and easily extensible, which makes the modeler's work more transparent. Furthermore, Stan's main inference engine, Hamiltonian Monte Carlo sampling, is amiable to diagnostics, which means the user can verify whether the obtained inference is reliable. In this tutorial, we demonstrate how to formulate, fit, and diagnose a compartmental transmission model in Stan, first with a simple susceptible-infected-recovered model, then with a more elaborate transmission model used during the SARS-CoV-2 pandemic. We also cover advanced topics which can further help practitioners fit sophisticated models; notably, how to use simulations to probe the model and priors, and computational techniques to scale-up models based on ordinary differential equations.},
	language = {en},
	number = {27},
	urldate = {2022-11-14},
	journal = {Statistics in Medicine},
	author = {Grinsztajn, Léo and Semenova, Elizaveta and Margossian, Charles C. and Riou, Julien},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.9164},
	keywords = {epidemiology, infectious diseases, Bayesian workflow, compartmental models},
	pages = {6209--6234},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/CPBRGQYS/Grinsztajn et al. - 2021 - Bayesian workflow for disease transmission modelin.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/YIKLLXAF/sim.html:text/html},
}


@article{xieMeasures2006,
	title = {Measures of {Bayesian} learning and identifiability in hierarchical models},
	volume = {136},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/pii/S0378375805001278},
	doi = {10.1016/j.jspi.2005.04.003},
	abstract = {Identifiability has long been an important concept in classical statistical estimation. Historically, Bayesians have been less interested in the concept since, strictly speaking, any parameter having a proper prior distribution also has a proper posterior, and is thus estimable. However, the larger statistical community's recent move toward more Bayesian thinking is largely fueled by an interest in Markov chain Monte Carlo-based analyses using vague or even improper priors. As such, Bayesians have been forced to think more carefully about what has been learned about the parameters of interest (given the data so far), or what could possibly be learned (given an infinite amount of data). In this paper, we propose measures of Bayesian learning based on differences in precision and Kullback–Leibler divergence. After investigating them in the context of some familiar Gaussian linear hierarchical models, we consider their use in a more challenging setting involving two sets of random effects (traditional and spatially arranged), only the sum of which is identified by the data. We illustrate this latter model with an example from periodontal data analysis, where the spatial aspect arises from the proximity of various measurements taken in the mouth. Our results suggest our measures behave sensibly and may be useful in even more complicated (e.g., non-Gaussian) model settings.},
	language = {en},
	number = {10},
	urldate = {2023-03-14},
	journal = {Journal of Statistical Planning and Inference},
	author = {Xie, Yang and Carlin, Bradley P.},
	month = oct,
	year = {2006},
	keywords = {Markov chain Monte Carlo (MCMC), Spatial statistics, Conditionally autoregressive (CAR) model, Nonidentifiability, Noninformativity},
	pages = {3458--3477},
}


@misc{kallioinenDetecting2022,
	title = {Detecting and diagnosing prior and likelihood sensitivity with power-scaling},
	url = {http://arxiv.org/abs/2107.14054},
	doi = {10.48550/arXiv.2107.14054},
	abstract = {Determining the sensitivity of the posterior to perturbations of the prior and likelihood is an important part of the Bayesian workflow. We introduce a practical and computationally efficient sensitivity analysis approach using importance sampling to estimate properties of posteriors resulting from power-scaling the prior or likelihood. On this basis, we suggest a diagnostic that can indicate the presence of prior-data conflict or likelihood noninformativity and discuss limitations to this power-scaling approach. The approach can be easily included in Bayesian workflows with minimal effort by the model builder and we present an implementation in our new R package priorsense. We further demonstrate the workflow on case studies of real data using models varying in complexity from simple linear models to Gaussian process models.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Kallioinen, Noa and Paananen, Topi and Bürkner, Paul-Christian and Vehtari, Aki},
	month = dec,
	year = {2022},
	note = {arXiv:2107.14054 [stat]},
	keywords = {Statistics - Methodology},
	annote = {Comment: 31 pages, 15 (+5 suppl) figures},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/FBFDYD2G/Kallioinen et al. - 2022 - Detecting and diagnosing prior and likelihood sens.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/QD6P6ALC/2107.html:text/html},
}


@article{ibrahimpower2015,
	title = {The power prior: theory and applications},
	volume = {34},
	issn = {1097-0258},
	shorttitle = {The power prior},
	doi = {10.1002/sim.6728},
	abstract = {The power prior has been widely used in many applications covering a large number of disciplines. The power prior is intended to be an informative prior constructed from historical data. It has been used in clinical trials, genetics, health care, psychology, environmental health, engineering, economics, and business. It has also been applied for a wide variety of models and settings, both in the experimental design and analysis contexts. In this review article, we give an A-to-Z exposition of the power prior and its applications to date. We review its theoretical properties, variations in its formulation, statistical contexts for which it has been used, applications, and its advantages over other informative priors. We review models for which it has been used, including generalized linear models, survival models, and random effects models. Statistical areas where the power prior has been used include model selection, experimental design, hierarchical modeling, and conjugate priors. Frequentist properties of power priors in posterior inference are established, and a simulation study is conducted to further examine the empirical performance of the posterior estimates with power priors. Real data analyses are given illustrating the power prior as well as the use of the power prior in the Bayesian design of clinical trials.},
	language = {eng},
	number = {28},
	journal = {Statistics in Medicine},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Gwon, Yeongjin and Chen, Fang},
	month = dec,
	year = {2015},
	pmid = {26346180},
	pmcid = {PMC4626399},
	keywords = {clinical trials, Research Design, Bayes Theorem, Models, Statistical, Linear Models, Statistics as Topic, Clinical Trials as Topic, Bayesian design, borrowing, discounting, historical data, Historically Controlled Study, informative prior},
	pages = {3724--3749},
	file = {Submitted Version:/home/bolker/Documents/zotero_new/storage/UHNBXBUC/Ibrahim et al. - 2015 - The power prior theory and applications.pdf:application/pdf},
}


@techreport{finkcompendium1997,
	title = {A compendium of conjugate priors},
	url = {https://web.archive.org/web/20090529203101/http://www.people.cornell.edu/pages/df36/CONJINTRnew%20TEX.pdf},
	author = {Fink, Daniel},
	year = {1997},
	note = {Publisher: Citeseer},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/24272TBY/Fink - 1997 - A compendium of conjugate priors.pdf:application/pdf},
}


@article{singmannStatistics2023,
	title = {Statistics in the {Service} of {Science}: {Don}’t {Let} the {Tail} {Wag} the {Dog}},
	volume = {6},
	issn = {2522-0861, 2522-087X},
	shorttitle = {Statistics in the {Service} of {Science}},
	url = {https://link.springer.com/10.1007/s42113-022-00129-2},
	doi = {10.1007/s42113-022-00129-2},
	abstract = {Statistical modeling is generally meant to describe patterns in data in service of the broader scientific goal of developing theories to explain those patterns. Statistical models support meaningful inferences when models are built so as to align parameters of the model with potential causal mechanisms and how they manifest in data. When statistical models are instead based on assumptions chosen by default, attempts to draw inferences can be uninformative or even paradoxical—in essence, the tail is trying to wag the dog. These issues are illustrated by van Doorn et al. (this issue) in the context of using Bayes Factors to identify effects and interactions in linear mixed models. We show that the problems identified in their applications (along with other problems identified here) can be circumvented by using priors over inherently meaningful units instead of default priors on standardized scales. This case study illustrates how researchers must directly engage with a number of substantive issues in order to support meaningful inferences, of which we highlight two: The first is the problem of coordination, which requires a researcher to specify how the theoretical constructs postulated by a model are functionally related to observable variables. The second is the problem of generalization, which requires a researcher to consider how a model may represent theoretical constructs shared across similar but non-identical situations, along with the fact that model comparison metrics like Bayes Factors do not directly address this form of generalization. For statistical modeling to serve the goals of science, models cannot be based on default assumptions, but should instead be based on an understanding of their coordination function and on how they represent causal mechanisms that may be expected to generalize to other related scenarios.},
	language = {en},
	number = {1},
	urldate = {2023-05-22},
	journal = {Computational Brain \& Behavior},
	author = {Singmann, Henrik and Kellen, David and Cox, Gregory E. and Chandramouli, Suyog H. and Davis-Stober, Clintin P. and Dunn, John C. and Gronau, Quentin F. and Kalish, Michael L. and McMullin, Sara D. and Navarro, Danielle J. and Shiffrin, Richard M.},
	month = mar,
	year = {2023},
	pages = {64--83},
	file = {Singmann et al. - 2023 - Statistics in the Service of Science Don’t Let th.pdf:/home/bolker/Documents/zotero_new/storage/VIC89R9H/Singmann et al. - 2023 - Statistics in the Service of Science Don’t Let th.pdf:application/pdf},
}


@article{gelmanPrior2006,
	title = {Prior distributions for variance parameters in hierarchical models (comment on article by {Browne} and {Draper})},
	volume = {1},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-3/Prior-distributions-for-variance-parameters-in-hierarchical-models-comment-on/10.1214/06-BA117A.full},
	doi = {10.1214/06-BA117A},
	abstract = {Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-\$t\$ family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of "noninformative" prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-\$t\$ family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-\$t\$ family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
	number = {3},
	urldate = {2021-06-12},
	journal = {Bayesian Analysis},
	author = {Gelman, Andrew},
	month = sep,
	year = {2006},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Bayesian inference, conditional conjugacy, folded-noncentral-\$t\$ distribution, half-\$t\$ distribution, hierarchical model, multilevel model, noninformative prior distribution, weakly informative prior distribution},
	pages = {515--534},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/JQPHQEVI/Gelman - 2006 - Prior distributions for variance parameters in hie.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/LX7F8HD5/06-BA117A.html:text/html},
}


@misc{carpenterComputational2017,
	title = {Computational and statistical issues with uniform interval priors},
	url = {http://andrewgelman.com/2017/11/28/computational-statistical-issues-uniform-interval-priors/},
	abstract = {There are two anti-patterns* for prior specification in Stan programs that can be sourced directly to idioms developed for BUGS. One is the diffuse gamma priors that Andrew’s already written about at length. The second is interval-based priors. Which brings us to today’s post. Interval priors An interval prior is something like this in Stan …},
	language = {en-US},
	urldate = {2018-05-15},
	journal = {Statistical Modeling, Causal Inference, and Social Science},
	author = {Carpenter, Bob},
	month = nov,
	year = {2017},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/UB6BV69E/computational-statistical-issues-uniform-interval-priors.html:text/html},
}


@book{inchaustiStatistical2023,
	title = {Statistical {Modeling} {With} {R}: a dual frequentist and {Bayesian} approach for life scientists},
	isbn = {978-0-19-267503-3},
	shorttitle = {Statistical {Modeling} {With} {R}},
	abstract = {To date, statistics has tended to be neatly divided into two theoretical approaches or frameworks: frequentist (or classical) and Bayesian. Scientists typically choose the statistical framework to analyse their data depending on the nature and complexity of the problem, and based on their personal views and prior training on probability and uncertainty. Although textbooks and courses should reflect and anticipate this dual reality, they rarely do so. This accessible textbook explains, discusses, and applies both the frequentist and Bayesian theoretical frameworks to fit the different types of statistical models that allow an analysis of the types of data most commonly gathered by life scientists. It presents the material in an informal, approachable, and progressive manner suitable for readers with only a basic knowledge of calculus and statistics. Statistical Modeling with R is aimed at senior undergraduate and graduate students, professional researchers, and practitioners throughout the life sciences, seeking to strengthen their understanding of quantitative methods and to apply them successfully to real world scenarios, whether in the fields of ecology, evolution, environmental studies, or computational biology.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Inchausti, Pablo},
	month = jan,
	year = {2023},
	note = {Google-Books-ID: uiyWEAAAQBAJ},
	keywords = {Computers / Database Administration \& Management, Computers / Mathematical \& Statistical Software, Mathematics / Applied, Mathematics / Probability \& Statistics / General, Science / Life Sciences / Taxonomy},
}


@article{lewandowskiGenerating2009a,
	title = {Generating random correlation matrices based on vines and extended onion method},
	volume = {100},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X09000876},
	doi = {10.1016/j.jmva.2009.04.008},
	abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276–294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177–2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions.},
	number = {9},
	urldate = {2017-08-04},
	journal = {Journal of Multivariate Analysis},
	author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
	month = oct,
	year = {2009},
	keywords = {Correlation matrix, Dependence vines, Onion method, Partial correlation},
	pages = {1989--2001},
	file = {ScienceDirect Full Text PDF:/home/bolker/Documents/zotero_new/storage/JZ36GIJN/Lewandowski et al. - 2009 - Generating random correlation matrices based on vi.pdf:application/pdf;ScienceDirect Snapshot:/home/bolker/Documents/zotero_new/storage/G4IX2BJ8/S0047259X09000876.html:text/html},
}


@inproceedings{sarmaPrior2020,
  title = {Prior Setting in Practice: Strategies and Rationales Used in Choosing Prior Distributions for {Bayesian} Analysis},
  shorttitle = {Prior {{Setting}} in {{Practice}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Sarma, Abhraneel and Kay, Matthew},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376377},
  urldate = {2023-05-27},
  abstract = {Bayesian statistical analysis is steadily growing in popularity and use. Choosing priors is an integral part of Bayesian inference. While there exist extensive normative recommendations for prior setting, little is known about how priors are chosen in practice. We conducted a survey (N = 50) and interviews (N = 9) where we used interactive visualizations to elicit prior distributions from researchers experienced withBayesian statistics and asked them for rationales for those priors. We found that participants' experience and philosophy influence how much and what information they are willing to incorporate into their priors, manifesting as different levels of informativeness and skepticism. We also identified three broad strategies participants use to set their priors: centrality matching, interval matching, and visual mass allocation. We discovered that participants' understanding of the notion of 'weakly informative priors"-a commonly-recommended normative approach to prior setting-manifests very differently across participants. Our results have implications both for how to develop prior setting recommendations and how to design tools to elicit priors in Bayesian analysis.},
  isbn = {978-1-4503-6708-0},
  keywords = {bayesian inference,descriptive analysis,prior distributions},
  file = {/home/bolker/Zotero/storage/S3K8XVIH/Sarma and Kay - 2020 - Prior Setting in Practice Strategies and Rational.pdf}
}

@article{vatsRevisiting2018,
  title = {Revisiting the {{Gelman-Rubin Diagnostic}}},
  author = {Vats, Dootika and Knudson, Christina},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.09384 [stat]},
  eprint = {1812.09384},
  primaryclass = {stat},
  urldate = {2019-01-29},
  abstract = {Gelman and Rubin's (1992) convergence diagnostic is one of the most popular methods for terminating a Markov chain Monte Carlo (MCMC) sampler. Since the seminal paper, researchers have developed sophisticated methods of variance estimation for Monte Carlo averages. We show that this class of estimators find immediate use in the Gelman-Rubin statistic, a connection not established in the literature before. We incorporate these estimators to upgrade both the univariate and multivariate Gelman-Rubin statistics, leading to increased stability in MCMC termination time. An immediate advantage is that our new Gelman-Rubin statistic can be calculated for a single chain. In addition, we establish a relationship between the Gelman-Rubin statistic and effective sample size. Leveraging this relationship, we develop a principled cutoff criterion for the Gelman-Rubin statistic. Finally, we demonstrate the utility of our improved diagnostic via examples.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Computation,Statistics - Methodology},
  file = {/home/bolker/Zotero/storage/39ZWMWD7/Vats and Knudson - 2018 - Revisiting the Gelman-Rubin Diagnostic.pdf;/home/bolker/Zotero/storage/9A2YQJRA/1812.html}
}

@article{lambertRobust2022,
  title = {{{R}}{${_\ast}$}: {{A Robust MCMC Convergence Diagnostic}} with {{Uncertainty Using Decision Tree Classifiers}}},
  shorttitle = {{{R}}{${_\ast}$}},
  author = {Lambert, Ben and Vehtari, Aki},
  year = {2022},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {17},
  number = {2},
  pages = {353--379},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1252},
  urldate = {2022-11-30},
  abstract = {Markov chain Monte Carlo (MCMC) has transformed Bayesian model inference over the past three decades: mainly because of this, Bayesian inference is now a workhorse of applied scientists. Under general conditions, MCMC sampling converges asymptotically to the posterior distribution, but this provides no guarantees about its performance in finite time. The predominant method for monitoring convergence is to run multiple chains and monitor individual chains' characteristics and compare these to the population as a whole: if within-chain and between-chain summaries are comparable, then this is taken to indicate that the chains have converged to a common stationary distribution. Here, we introduce a new method for diagnosing convergence based on how well a machine learning classifier model can successfully discriminate the individual chains. We call this convergence measure R{${_\ast}$}. In contrast to the predominant R\textasciicircum, R{${_\ast}$} is a single statistic across all parameters that indicates lack of mixing, although individual variables' importance for this metric can also be determined. Additionally, R{${_\ast}$} is not based on any single characteristic of the sampling distribution; instead it uses all the information in the chain, including that given by the joint sampling distribution, which is currently largely overlooked by existing approaches. We recommend calculating R{${_\ast}$} using two different machine learning classifiers \textemdash{} gradient-boosted regression trees and random forests \textemdash{} which each work well in models of different dimensions. Because each of these methods outputs a classification probability, as a byproduct, we obtain uncertainty in R{${_\ast}$}. The method is straightforward to implement and could be a complementary additional check on MCMC convergence for applied analyses.},
  file = {/home/bolker/Zotero/storage/RX9L2CNB/Lambert and Vehtari - 2022 - R∗ A Robust MCMC Convergence Diagnostic with Unce.pdf;/home/bolker/Zotero/storage/ITVLSUIL/20-BA1252.html}
}

@article{vehtariRankNormalization2021a,
  title = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}: {{An Improved R-hat}} for {{Assessing Convergence}} of {{MCMC}} (with {{Discussion}})},
  shorttitle = {Rank-{{Normalization}}, {{Folding}}, and {{Localization}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2021},
  month = jun,
  journal = {Bayesian Analysis},
  volume = {16},
  number = {2},
  pages = {667--718},
  publisher = {{International Society for Bayesian Analysis}},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/20-BA1221},
  urldate = {2022-04-18},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic R\textasciicircum{} of Gelman and Rubin (1992) has serious flaws. Traditional R\textasciicircum{} will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice.},
  file = {/home/bolker/Zotero/storage/9LQLBVA6/Vehtari et al. - 2021 - Rank-Normalization, Folding, and Localization An .pdf;/home/bolker/Zotero/storage/KW8BZVYG/20-BA1221.html}
}


@misc{Best2019,
  title = {N Best Tips \& Tricks (or the Go-to Checklist) for New {{Stan}} Model Builders?},
  year = {2019},
  month = may,
  journal = {The Stan Forums},
  urldate = {2023-05-29},
  abstract = {Is there a one-page checklist of the captioned written somewhere?  In my recent exploration as a newbie (see this post), I find the following tricks most useful:    specify any known bounds of a parameter (thereby help Stan to pick the appropriate default prior, eg, uniform vs. normal)    explicitly specify a prior for a parameter that better reflects your knowledge about it than Stan's default prior choice could possibly represent in general    use non-centered parametrization EDIT: (eg, to fix...},
  chapter = {Modeling},
  howpublished = {https://discourse.mc-stan.org/t/n-best-tips-tricks-or-the-go-to-checklist-for-new-stan-model-builders/8688},
  langid = {english},
  file = {/home/bolker/Zotero/storage/RMKP3ZVN/8688.html}
}



@article{reimherrPrior2021,
	title = {Prior {Sample} {Size} {Extensions} for {Assessing} {Prior} {Impact} and {Prior}-{Likelihood} {Discordance}},
	volume = {83},
	issn = {1369-7412},
	url = {https://doi.org/10.1111/rssb.12414},
	doi = {10.1111/rssb.12414},
	abstract = {This paper outlines a framework for quantifying the prior’s contribution to posterior inference in the presence of prior-likelihood discordance, a broader concept than the usual notion of prior-likelihood conflict. We achieve this dual purpose by extending the classic notion of prior sample size, M, in three directions: (I) estimating M beyond conjugate families; (II) formulating M as a relative notion that is as a function of the likelihood sample size k, M(k), which also leads naturally to a graphical diagnosis; and (III) permitting negative M, as a measure of prior-likelihood conflict, that is, harmful discordance. Our asymptotic regime permits the prior sample size to grow with the likelihood data size, hence making asymptotic arguments meaningful for investigating the impact of the prior relative to that of likelihood. It leads to a simple asymptotic formula for quantifying the impact of a proper prior that only involves computing a centrality and a spread measure of the prior and the posterior. We use simulated and real data to illustrate the potential of the proposed framework, including quantifying how weak is a ‘weakly informative’ prior adopted in a study of lupus nephritis. Whereas we take a pragmatic perspective in assessing the impact of a prior on a given inference problem under a specific evaluative metric, we also touch upon conceptual and theoretical issues such as using improper priors and permitting priors with asymptotically non-vanishing influence.},
	number = {3},
	urldate = {2023-05-29},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Reimherr, Matthew and Meng, Xiao-Li and Nicolae, Dan L.},
	month = jul,
	year = {2021},
	pages = {413--437},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/Z77GJTSZ/Reimherr et al. - 2021 - Prior Sample Size Extensions for Assessing Prior I.pdf:application/pdf},
}

@article{mullerMeasuring2012,
	title = {Measuring prior sensitivity and prior informativeness in large {Bayesian} models},
	volume = {59},
	issn = {03043932},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S030439321200092X},
	doi = {10.1016/j.jmoneco.2012.09.003},
	abstract = {In large Bayesian models, such as modern DSGE models, it is difﬁcult to assess how much the prior affects the results. This paper derives measures of prior sensitivity and prior informativeness that account for the high dimensional interaction between prior and likelihood information. The basis for both measures is the derivative matrix of the posterior mean with respect to the prior mean, which is easily obtained from Markov Chain Monte Carlo output. We illustrate the approach by examining posterior results in the small model of Lubik and Schorfheide (2004) and the large model of Smets and Wouters (2007).},
	language = {en},
	number = {6},
	urldate = {2023-05-29},
	journal = {Journal of Monetary Economics},
	author = {Müller, Ulrich K.},
	month = oct,
	year = {2012},
	pages = {581--597},
	file = {Müller - 2012 - Measuring prior sensitivity and prior informativen.pdf:/home/bolker/Documents/zotero_new/storage/IRAKCGAK/Müller - 2012 - Measuring prior sensitivity and prior informativen.pdf:application/pdf},
}



@techreport{lindleyBayesian1980,
	title = {The {Bayesian} {Approach} to {Statistics}},
	url = {https://apps.dtic.mil/sti/pdfs/ADA087836.pdf},
	language = {en},
	number = {ORC 80-9},
	institution = {Operations Research Center, University of California, Berkeley},
	author = {Lindley, Dennis V},
	month = may,
	year = {1980},
	file = {Lindley - THE BAYESIAN APPROACH TO STATISTICS.pdf:/home/bolker/Documents/zotero_new/storage/TSW9E7Y7/Lindley - THE BAYESIAN APPROACH TO STATISTICS.pdf:application/pdf},
}

@article{chungNondegenerate2013a,
  title = {A {{Nondegenerate Penalized Likelihood Estimator}} for {{Variance Parameters}} in {{Multilevel Models}}},
  author = {Chung, Yeojin and {Rabe-Hesketh}, Sophia and Dorie, Vincent and Gelman, Andrew and Liu, Jingchen},
  year = {2013},
  month = mar,
  journal = {Psychometrika},
  volume = {78},
  number = {4},
  pages = {685--709},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/s11336-013-9328-2},
  urldate = {2015-07-15},
  abstract = {Group-level variance estimates of zero often arise when fitting multilevel or hierarchical linear models, especially when the number of groups is small. For situations where zero variances are implausible a priori, we propose a maximum penalized likelihood approach to avoid such boundary estimates. This approach is equivalent to estimating variance parameters by their posterior mode, given a weakly informative prior distribution. By choosing the penalty from the log-gamma family with shape parameter greater than 1, we ensure that the estimated variance will be positive. We suggest a default log-gamma(2,{$\lambda$}) penalty with {$\lambda\rightarrow$}0, which ensures that the maximum penalized likelihood estimate is approximately one standard error from zero when the maximum likelihood estimate is zero, thus remaining consistent with the data while being nondegenerate. We also show that the maximum penalized likelihood estimator with this default penalty is a good approximation to the posterior median obtained under a noninformative prior. Our default method provides better estimates of model parameters and standard errors than the maximum likelihood or the restricted maximum likelihood estimators. The log-gamma family can also be used to convey substantive prior information. In either case\textemdash pure penalization or prior information\textemdash our recommended procedure gives nondegenerate estimates and in the limit coincides with maximum likelihood as the number of groups increases.},
  langid = {english},
  keywords = {{Assessment, Testing and Evaluation},Bayes modal estimation,hierarchical linear model,Mixed Model,Multilevel model,penalized likelihood,Psychometrics,Statistical Theory and Methods,{Statistics for Social Science, Behavorial Science, Education, Public Policy, and Law},variance estimation,weakly informative prior},
  file = {/home/bolker/Zotero/storage/D5QRECWM/Chung et al_2013_A Nondegenerate Penalized Likelihood Estimator for Variance Parameters in.pdf;/home/bolker/Zotero/storage/BK6MIVBC/10.html}
}

@book{gelmanData2006,
  title = {Data {{Analysis Using Regression}} and {{Multilevel}}/{{Hierarchical Models}}},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2006 xx xx},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, England}},
  keywords = {uploaded}
}

@article{schielzethSimple2010,
  title = {Simple Means to Improve the Interpretability of Regression Coefficients: {{Interpretation}} of Regression Coefficients},
  shorttitle = {Simple Means to Improve the Interpretability of Regression Coefficients},
  author = {Schielzeth, Holger},
  year = {2010},
  month = feb,
  journal = {Methods in Ecology and Evolution},
  volume = {1},
  number = {2},
  pages = {103--113},
  issn = {2041210X, 2041210X},
  doi = {10.1111/j.2041-210X.2010.00012.x},
  urldate = {2016-06-08},
  langid = {english},
  file = {/home/bolker/Zotero/storage/XNG3ZF5X/j.2041-210X.2010.00012.x.pdf}
}


@article{wanEstimating2014,
  title = {Estimating the Sample Mean and Standard Deviation from the Sample Size, Median, Range and/or Interquartile Range},
  author = {Wan, Xiang and Wang, Wenqian and Liu, Jiming and Tong, Tiejun},
  year = {2014},
  month = dec,
  journal = {BMC Medical Research Methodology},
  volume = {14},
  number = {1},
  pages = {135},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-135},
  urldate = {2023-06-02},
  abstract = {In systematic reviews and meta-analysis, researchers often pool the results of the sample mean and standard deviation from a set of similar clinical trials. A number of the trials, however, reported the study using the median, the minimum and maximum values, and/or the first and third quartiles. Hence, in order to combine results, one may have to estimate the sample mean and standard deviation for such trials.},
  langid = {english},
  keywords = {Interquartile range,Median,Meta-analysis,Sample mean,Sample size,Standard deviation},
  file = {/home/bolker/Zotero/storage/TLREHG4J/Wan et al. - 2014 - Estimating the sample mean and standard deviation .pdf}
}

@article{bolkerStrategies2013,
  title = {Strategies for Fitting Nonlinear Ecological Models in {{R}}, {{AD Model Builder}}, and {{BUGS}}},
  author = {Bolker, Benjamin M. and Gardner, Beth and Maunder, Mark and Berg, Casper W. and Brooks, Mollie and Comita, Liza and Crone, Elizabeth and Cubaynes, Sarah and Davies, Trevor and {de Valpine}, Perry and Ford, Jessica and Gimenez, Olivier and K{\'e}ry, Marc and Kim, Eun Jung and {Lennert-Cody}, Cleridy and Magnusson, Arni and Martell, Steve and Nash, John and Nielsen, Anders and Regetz, Jim and Skaug, Hans and Zipkin, Elise},
  editor = {Ramula, Satu},
  year = {2013},
  month = jun,
  journal = {Methods in Ecology and Evolution},
  volume = {4},
  number = {6},
  pages = {501--512},
  issn = {2041210X},
  doi = {10.1111/2041-210X.12044},
  urldate = {2013-06-11},
  file = {/home/bolker/Zotero/storage/U83CEWVU/Bolker et al_2013_Strategies for fitting nonlinear ecological models in R, AD Model Builder, and.pdf}
}
