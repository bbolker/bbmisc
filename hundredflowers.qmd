---
title: "brain dump on convergence of model-fitting toolboxes"
date: today
---

This is a brain dump, perhaps/hopefully to be turned into a review paper eventually.

Attending ISEC 2024 in Swansea recently, I was very excited about what appears to be a significant convergence of mid-level toolboxes for constructing complex models for ecological data. A medium-sized handful of platforms are now available for building and efficiently estimating models including various kinds of correlation structure (spatial, temporal, phylogenetic), complex random effects, smooth nonlinear terms, and a wide range of conditional distributions. Equally exciting, these platforms are becoming more interoperable, allowing the use of structures that have been built and tested on one platform within another. 

I like the idea of ["let a hundred flowers bloom"](https://en.wikipedia.org/wiki/Hundred_Flowers_Campaign) to represent this explosion of diversity, although the historical Maoist original is a bad model (ended in an ideological crackdown). I will let go of this idea eventually.

I would like to think we are gradually approaching the nirvana where modeling is nearly frictionless; I can decide what model makes sense for my problem and easily implement it in any one of a number of platforms. (Tools for 

Some ideas in no particular order:

* **scope**: the platforms I'm thinking of are

- `TMB`, `RTMB`: autodiff + Laplace approximation. Connectors: `tmbstan`. Procedural. Base language C++ (`TMB`), R (`RTMB`)
- `nimble`: Gibbs sampling, HMC, SMC ... autodiff. Base language R. Graphical.
- `INLA`: nested Laplace approximation. Formula. Connectors: `fmesher`.
- `greta`: autodiff via PyTorch. Base language R (via Python?). Procedural.
- `Stan`: (`rethinking`, `brms`): autodiff, HMC (improved NUTS) sampling. Base language C++. Procedural (`rethinking::ulam()` is graphical; `brms` and `rstanarm` are formula).
- `mgcv`: (`gamm4`, `scam`): exportable smooth bases/penalty matrix. Formula.
- `JAGS`: Gibbs sampling. Graphical.

## Bayesian vs frequentist

These boundaries are progressively blurring. Traditional frequentist mixed models use *empirical Bayesian* estimation. That is, they use shrinkage estimators (joint priors to a Bayesian) at intermediate levels of the model, but without imposing any priors at the top level of the model. When frequentist models further include some kind of regularization (e.g. a ridge penalty on fixed effect coefficients), they look even more Bayesian. The important remaining distinction is how inference, and especially assessment of uncertainty, is done; frequentists find the maximum likelihood (what would be maximum *a posteriori* estimation to a Bayesian), and often use plug-in estimates when computing uncertainty (e.g., conditioning on the estimated value of covariance parameters rather than allowing for their uncertainty).

More to the point, for a given model a hurried Bayesian might choose to use optimization to find the mode quickly, using a full posterior sample only once initial modeling challenges are addressed. (Bayesian concerns about the non-representativeness of the mode in high-dimensional spaces are less concerning if the top-level parameters of a hierarchical model are relatively low-dimensional, and we are using something like Laplace approximation to integrate over the high-dimensional latent variables at lower levels in the model.) Conversely, an otherwise MLE-loving researcher might choose to plug their empirical-Bayes+regularization model into a Hamiltonian Monte Carlo engine to get better uncertainty estimates of derived quantities than are available via (say) the generalized delta method.

## Interface

* **Procedural**: user writes an objective or loss function, typically a negative log-likelihood or (negative or positive) log-prior function. This is the 'lowest-level' and most flexible approach, but contains the least semantic/higher-level information about problem structure (e.g. `TMB`, `Stan`)
* **Graphical**: user writes the model definition in terms of distributional assumptions (`X ~ distrib(...)`) as well as deterministic computations ("logical nodes", `X <- ...`). The platform typically uses this to construct a directed acyclic graph and determine appropriate samplers (e.g. `JAGS`, `nimble`, `greta`)
* **Formula**: user writes a higher-level form of the model as a formula in extended Bates-Wilkinson-Rogers notation, typically with a separate `family` argument to determine the conditional distribution (e.g. `brms`, `INLA`)

- there may be some degree of hybridization between the procedural and graphical level (e.g. `Stan` uses `~` and RTMB has a `%~%` operator for distributional/stochastic nodes)
- both autodiff and Gibbs samplers construct a DAG, although at different levels (call graph/tape vs. sampling sequence)
- Many low-/mid-level platforms have formula-based front ends that handle a narrower problem scope (`TMB`/`glmmTMB`, `Stan`/`brms`, `JAGS`/`runjags::template.jags()`, etc.)

```{r pkgs, echo = FALSE}
a1 <- available.packages()
re_setdiff <- function(x, y, ...) {
   for (yy in y) {
      x <- grep(yy, x, invert = TRUE, value = TRUE, ...)
   }
   return(x)
}
find_pkgs <- function(pkg, ignore = character(0),
                      add = character(0),
                      edge = TRUE, verbose=TRUE) {
    if (verbose) cat("Packages using ", pkg, ":\n",
                     sep = "")
    ## always ignore base package
    ignore <- c(sprintf("^%s$",pkg), ignore)
    ## only look at beginning and end of string
    if (edge) {
        pkg <- sprintf("(^%s|%s$)", pkg, pkg)
    }
    grep(pkg, rownames(a1), ignore.case = TRUE, value = TRUE) |> re_setdiff(ignore)
}
find_pkgs("TMB", ignore = c("RTMB", "tmbstan"), add = "gllvm")
find_pkgs("INLA")
find_pkgs("nimble")
## are 
find_pkgs("jags", IGNORE = c("rjags", "R2jags"))
find_pkgs("stan", ignore = c("standard", "StanHeaders", "rstan"))
```

## Connections

- `reformulas` for random effect formula processing
- `fmesher` for SPDE meshes
- `mgcv::smoothCon`, `mgcv::smooth2random` for exporting bases
- `tmbstan` for Stan-based HMC with `TMB` models
- `mgcv::jagam` for JAGS models with `mgcv` smooths

## Methods

- autodiff (Kristensen et al, Griewank) (`TMB`, `Stan`)
- sparse methods (`lme4`, `TMB`, `INLA`)
- low-rank approximation (`mgcv`)
- basis expansion (Wood, Hefley et al, Hodges) (`mgcv`)
- Gibbs sampling (`JAGS`, `nimble`)
- Laplace approximation, quadrature (importance sampling, ...) (`TMB`, `nimble`, ... ?)
- INLA

