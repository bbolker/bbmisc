
@article{meshkatIdentifiability2015,
	title = {Identifiability results for several classes of linear compartment models},
	volume = {77},
	number = {8},
	journal = {Bulletin of mathematical biology},
	author = {Meshkat, Nicolette and Sullivant, Seth and Eisenberg, Marisa},
	year = {2015},
	pages = {1620--1651},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/EYRYF6PA/s11538-015-0098-0.html:text/html},
}


@article{griewankIntroduction2003,
	title = {Introduction to {Automatic} {Differentiation}},
	volume = {2},
	copyright = {Copyright © 2003 WILEY‐VCH Verlag GmbH \& Co. KGaA, Weinheim},
	issn = {1617-7061},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pamm.200310012},
	doi = {10.1002/pamm.200310012},
	abstract = {Automatic, or algorithmic, differentiation (AD) is a chain rule-based technique for evaluating derivatives of functions given as computer programs for their elimination. We review the main characteristics and application of AD and illustrate the methodology on a simple example.},
	language = {en},
	number = {1},
	urldate = {2020-09-18},
	journal = {PAMM},
	author = {Griewank, Andreas and Walther, Andrea},
	year = {2003},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/pamm.200310012},
	pages = {45--49},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/WMFYIFDJ/Griewank and Walther - 2003 - Introduction to Automatic Differentiation.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/PABDERVX/pamm.html:text/html},
}

@article{griewankautomatic1989a,
	title = {On automatic differentiation},
	journal = {Mathematical programming: recent developments and applications},
	author = {Griewank, A.},
	year = {1989},
	pages = {83--108},
}

@incollection{griewankautomatic1989,
	title = {On automatic differentiation},
	volume = {6},
	url = {http://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR89003.pdf},
	booktitle = {Mathematical {Programming}: recent developments and applications},
	publisher = {Kluwer},
	author = {Griewank, Andreas},
	editor = {Iri, M. and Tanabe, K.},
	year = {1989},
	pages = {83--108},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/CKER7HXN/Griewank - 1989 - On automatic differentiation.pdf:application/pdf},
}

@article{griewankAchieving1992,
	title = {Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation},
	volume = {1},
	number = {1},
	journal = {Optimization Methods and software},
	author = {Griewank, Andreas},
	year = {1992},
	pages = {35--54},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/3Y38DS5Z/Griewank - 1992 - Achieving logarithmic growth of temporal and spati.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/BSNTYF62/10556789208805505.html:text/html},
}


@article{kristensenTMB2016,
	title = {{TMB} : {Automatic} {Differentiation} and \{{Laplace}\} {Approximation}},
	volume = {70},
	issn = {1548-7660},
	shorttitle = {\textbf{{TMB}}},
	url = {http://www.jstatsoft.org/v70/i05/},
	doi = {10.18637/jss.v070.i05},
	language = {en},
	number = {5},
	urldate = {2017-05-23},
	journal = {Journal of Statistical Software},
	author = {Kristensen, Kasper and Nielsen, Anders and Berg, Casper W. and Skaug, Hans and Bell, Bradley M.},
	year = {2016},
	file = {v70i05.pdf:/home/bolker/Documents/zotero_new/storage/B35XN3DP/v70i05.pdf:application/pdf},
}

@article{monnahanNoUturn2018,
	title = {No-{U}-turn sampling for fast {Bayesian} inference in {ADMB} and {TMB}: {Introducing} the adnuts and tmbstan {R} packages},
	volume = {13},
	shorttitle = {No-{U}-turn sampling for fast {Bayesian} inference in {ADMB} and {TMB}},
	number = {5},
	journal = {PloS one},
	author = {Monnahan, Cole C. and Kristensen, Kasper},
	year = {2018},
	pages = {e0197954},
	file = {Full Text:/home/bolker/Documents/zotero_new/storage/W29XEFSF/article.html:text/html},
}

@article{brooksglmmTMB2017,
	title = {{glmmTMB} balances speed and flexibility among packages for zero-inflated generalized linear mixed modeling},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2073-4859},
	url = {https://www.research-collection.ethz.ch/handle/20.500.11850/242692},
	doi = {10.3929/ethz-b-000240890},
	abstract = {Count data can be analyzed using generalized linear mixed models when observations are correlated in ways that require random effects. However, count data are often zero-inflated, containing more zeros than would be expected from the typical error distributions. We present a new package, glmmTMB, and compare it to other R packages that fit zero-inflated mixed models. The glmmTMB package fits many types of GLMMs and extensions, including models with continuously distributed responses, but here we focus on count responses. glmmTMB is faster than glmmADMB, MCMCglmm, and brms, and more flexible than INLA and mgcv for zero-inflated modeling. One unique feature of glmmTMB (among packages that fit zero-inflated mixed models) is its ability to estimate the Conway-Maxwell-Poisson distribution parameterized by the mean. Overall, its most appealing features for new users may be the combination of speed, flexibility, and its interface’s similarity to lme4.},
	language = {en},
	number = {2},
	urldate = {2024-02-24},
	journal = {The R journal},
	author = {Brooks, Mollie E. and Kristensen, Kasper and van Benthem, Koen J. and Magnusson, Arni and Berg, Casper W. and Nielsen, Anders and Skaug, Hans J. and Machler, Martin and Bolker, Benjamin M.},
	month = dec,
	year = {2017},
	note = {Accepted: 2018-02-19T11:14:34Z
Publisher: Technische Universitaet Wien},
	pages = {378--400},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/KEXZRS2W/Brooks et al. - 2017 - glmmTMB balances speed and flexibility among packa.pdf:application/pdf},
}


@article{hefleybasis2017,
	title = {The basis function approach for modeling autocorrelation in ecological data},
	volume = {98},
	issn = {1939-9170},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ecy.1674},
	doi = {10.1002/ecy.1674},
	abstract = {Analyzing ecological data often requires modeling the autocorrelation created by spatial and temporal processes. Many seemingly disparate statistical methods used to account for autocorrelation can be expressed as regression models that include basis functions. Basis functions also enable ecologists to modify a wide range of existing ecological models in order to account for autocorrelation, which can improve inference and predictive accuracy. Furthermore, understanding the properties of basis functions is essential for evaluating the fit of spatial or time-series models, detecting a hidden form of collinearity, and analyzing large data sets. We present important concepts and properties related to basis functions and illustrate several tools and techniques ecologists can use when modeling autocorrelation in ecological data.},
	language = {en},
	number = {3},
	urldate = {2022-07-14},
	journal = {Ecology},
	author = {Hefley, Trevor J. and Broms, Kristin M. and Brost, Brian M. and Buderman, Frances E. and Kay, Shannon L. and Scharf, Henry R. and Tipton, John R. and Williams, Perry J. and Hooten, Mevin B.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ecy.1674},
	keywords = {time series, spatial statistics, autocorrelation, collinearity, Bayesian model, dimension reduction, semiparametric regression},
	pages = {632--646},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/L7EU9K59/Hefley et al. - 2017 - The basis function approach for modeling autocorre.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/KG7C6YGR/ecy.html:text/html},
}


@book{woodGeneralized2017,
	series = {{CRC} {Texts} in {Statistical} {Science}},
	title = {Generalized {Additive} {Models}: {An} {Introduction} with {R}},
	urldate = {2017-11-28},
	publisher = {Chapman \& Hall},
	author = {Wood, Simon N.},
	year = {2017}
}

@article{woodconfidence2006,
	title = {On confidence intervals for generalized additive models based on penalized regression splines},
	volume = {48},
	issn = {1369-1473},
	url = {https://journals.scholarsportal.info/details/13691473/v48i0004/445_ocifgamboprs.xml},
	doi = {10.1111/j.1467-842X.2006.00450.x},
	abstract = {Summary Generalized additive models represented using low rank penalized regression splines, estimated by penalized likelihood maximisation and with smoothness selected by generalized cross validation or similar criteria, provide a computationally efficient general framework for practical smooth modelling. Various authors have proposed approximate Bayesian interval estimates for such models, based on extensions of the work of [Bayesian confidence intervals for the cross validated smoothing spline. J. R. Statist. Soc. B45, 133–150] and [Some aspects of the spline smoothing approach to nonparametric regression curve fitting. J. R. Statist. Soc. B47, 1–52] on smoothing spline models of Gaussian data, but testing of such intervals has been rather limited and there is little supporting theory for the approximations used in the generalized case. This paper aims to improve this situation by providing simulation tests and obtaining asymptotic results supporting the approximations employed for the generalized case. The simulation results suggest that while across-the-model performance is good, component-wise coverage probabilities are not as reliable. Since this is likely to result from the neglect of smoothing parameter variability, a simple and efficient simulation method is proposed to account for smoothing parameter uncertainty: this is demonstrated to substantially improve the performance of component-wise intervals.},
	number = {4},
	urldate = {2021-12-02},
	journal = {Australian \& New Zealand Journal of Statistics},
	author = {Wood, Simon N.},
	year = {2006},
	note = {Publisher: Blackwell Publishing Asia},
	keywords = {Bayesian confidence interval, generalized additive model (GAM), generalized cross validation (GCV), multiple smoothing parameters, penalized regression spline},
	pages = {445--464},
	file = {Scholars Portal Full Text PDF:/home/bolker/Documents/zotero_new/storage/4PNVL963/Wood - 2006 - ON CONFIDENCE INTERVALS FOR GENERALIZED ADDITIVE M.pdf:application/pdf},
}

@article{woodFast2011,
	title = {Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models},
	volume = {73},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00749.x},
	doi = {10.1111/j.1467-9868.2010.00749.x},
	abstract = {Summary. Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton–Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection.},
	language = {en},
	number = {1},
	urldate = {2023-02-07},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Wood, Simon N.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2010.00749.x},
	keywords = {Model selection, Marginal likelihood, Restricted maximum likelihood, Generalized additive mixed model, Generalized additive model, Generalized cross-validation, Adaptive smoothing, Penalized generalized linear model, Penalized regression splines, Scalar on function regression, Stable computation},
	pages = {3--36},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/STZE99H9/Wood - 2011 - Fast stable restricted maximum likelihood and marg.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/YBY9GSA9/j.1467-9868.2010.00749.html:text/html},
}

@article{pyaShape2015,
	title = {Shape constrained additive models},
	volume = {25},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-013-9448-7},
	doi = {10.1007/s11222-013-9448-7},
	abstract = {A framework is presented for generalized additive modelling under shape constraints on the component functions of the linear predictor of the GAM. We represent shape constrained model components by mildly non-linear extensions of P-splines. Models can contain multiple shape constrained and unconstrained terms as well as shape constrained multi-dimensional smooths. The constraints considered are on the sign of the first or/and the second derivatives of the smooth terms. A key advantage of the approach is that it facilitates efficient estimation of smoothing parameters as an integral part of model estimation, via GCV or AIC, and numerically robust algorithms for this are presented. We also derive simulation free approximate Bayesian confidence intervals for the smooth components, which are shown to achieve close to nominal coverage probabilities. Applications are presented using real data examples including the risk of disease in relation to proximity to municipal incinerators and the association between air pollution and health.},
	language = {en},
	number = {3},
	urldate = {2023-10-18},
	journal = {Statistics and Computing},
	author = {Pya, Natalya and Wood, Simon N.},
	month = may,
	year = {2015},
	keywords = {Generalized additive model, Convex smoothing, Monotonic smoothing, P-splines},
	pages = {543--559},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/HY75ERFU/Pya and Wood - 2015 - Shape constrained additive models.pdf:application/pdf},
}


@article{leleEstimability2010a,
	title = {Estimability and {Likelihood} {Inference} for {Generalized} {Linear} {Mixed} {Models} {Using} {Data} {Cloning}},
	volume = {105},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/abs/10.1198/jasa.2010.tm09757},
	doi = {10.1198/jasa.2010.tm09757},
	number = {492},
	urldate = {2014-07-31},
	journal = {Journal of the American Statistical Association},
	author = {Lele, Subhash R. and Nadeem, Khurram and Schmuland, Byron},
	month = dec,
	year = {2010},
	pages = {1617--1625},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/9V5I5QSW/jasa.2010.html:text/html},
}


@article{maclarenWhat2020,
	title = {What can be estimated? {Identifiability}, estimability, causal inference and ill-posed inverse problems},
	shorttitle = {What can be estimated?},
	url = {http://arxiv.org/abs/1904.02826},
	abstract = {We consider basic conceptual questions concerning the relationship between statistical estimation and causal inference. Firstly, we show how to translate causal inference problems into an abstract statistical formalism without requiring any structure beyond an arbitrarily-indexed family of probability models. The formalism is simple but can incorporate a variety of causal modelling frameworks, including 'structural causal models', but also models expressed in terms of, e.g., differential equations. We focus primarily on the structural/graphical causal modelling literature, however. Secondly, we consider the extent to which causal and statistical concerns can be cleanly separated, examining the fundamental question: 'What can be estimated from data?'. We call this the problem of estimability. We approach this by analysing a standard formal definition of 'can be estimated' commonly adopted in the causal inference literature -- identifiability -- in our abstract statistical formalism. We use elementary category theory to show that identifiability implies the existence of a Fisher-consistent estimator, but also show that this estimator may be discontinuous, and thus unstable, in general. This difficulty arises because the causal inference problem is, in general, an ill-posed inverse problem. Inverse problems have three conditions which must be satisfied to be considered well-posed: existence, uniqueness, and stability of solutions. Here identifiability corresponds to the question of uniqueness; in contrast, we take estimability to mean satisfaction of all three conditions, i.e. well-posedness. Lack of stability implies that naive translation of a causally identifiable quantity into an achievable statistical estimation target may prove impossible. Our article is primarily expository and aimed at unifying ideas from multiple fields, though we provide new constructions and proofs.},
	urldate = {2021-06-23},
	journal = {arXiv:1904.02826 [cs, math, stat]},
	author = {Maclaren, Oliver J. and Nicholson, Ruanui},
	month = jul,
	year = {2020},
	note = {arXiv: 1904.02826},
	keywords = {Mathematics - Statistics Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 41 pages, 5 figures. Fixed typos, added references, added examples. New examples (updated again) introduce explicit 'view' and 'undo' operations to complement 'do' operation as part of the translation between structural causal models and our abstract statistical formalism},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/RYL5DG34/Maclaren and Nicholson - 2020 - What can be estimated Identifiability, estimabili.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/F5BKMQG4/1904.html:text/html},
}

@article{simpsonMaking2024,
	title = {Making {Predictions} {Using} {Poorly} {Identified} {Mathematical} {Models}},
	volume = {86},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/s11538-024-01294-0},
	doi = {10.1007/s11538-024-01294-0},
	abstract = {Many commonly used mathematical models in the field of mathematical biology involve challenges of parameter non-identifiability. Practical non-identifiability, where the quality and quantity of data does not provide sufficiently precise parameter estimates is often encountered, even with relatively simple models. In particular, the situation where some parameters are identifiable and others are not is often encountered. In this work we apply a recent likelihood-based workflow, called Profile-Wise Analysis (PWA), to non-identifiable models for the first time. The PWA workflow addresses identifiability, parameter estimation, and prediction in a unified framework that is simple to implement and interpret. Previous implementations of the workflow have dealt with idealised identifiable problems only. In this study we illustrate how the PWA workflow can be applied to both structurally non-identifiable and practically non-identifiable models in the context of simple population growth models. Dealing with simple mathematical models allows us to present the PWA workflow in a didactic, self-contained document that can be studied together with relatively straightforward Julia code provided on GitHub. Working with simple mathematical models allows the PWA workflow prediction intervals to be compared with gold standard full likelihood prediction intervals. Together, our examples illustrate how the PWA workflow provides us with a systematic way of dealing with non-identifiability, especially compared to other approaches, such as seeking ad hoc parameter combinations, or simply setting parameter values to some arbitrary default value. Importantly, we show that the PWA workflow provides insight into the commonly-encountered situation where some parameters are identifiable and others are not, allowing us to explore how uncertainty in some parameters, and combinations of parameters, regardless of their identifiability status, influences model predictions in a way that is insightful and interpretable.},
	language = {en},
	number = {7},
	urldate = {2024-06-17},
	journal = {Bulletin of Mathematical Biology},
	author = {Simpson, Matthew J. and Maclaren, Oliver J.},
	month = may,
	year = {2024},
	keywords = {Parameter estimation, Profile likelihood, Parameter identifiability, Model prediction},
	pages = {80},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/DWI85RDC/Simpson and Maclaren - 2024 - Making Predictions Using Poorly Identified Mathema.pdf:application/pdf},
}

@book{coleParameter2020,
	address = {Boca Raton London New York, NY},
	edition = {1st edition},
	title = {Parameter {Redundancy} and {Identifiability}},
	isbn = {978-1-4987-2087-8},
	abstract = {Statistical and mathematical models are defined by parameters that describe different characteristics of those models. Ideally it would be possible to find parameter estimates for every parameter in that model, but, in some cases, this is not possible. For example, two parameters that only ever appear in the model as a product could not be estimated individually; only the product can be estimated. Such a model is said to be parameter redundant, or the parameters are described as non-identifiable. This book explains why parameter redundancy and non-identifiability is a problem and the different methods that can be used for detection, including in a Bayesian context. Key features of this book:Detailed discussion of the problems caused by parameter redundancy and non-identifiabilityExplanation of the different general methods for detecting parameter redundancy and non-identifiability, including symbolic algebra and numerical methodsChapter on Bayesian identifiabilityThroughout illustrative examples are used to clearly demonstrate each problem and method. Maple and R code are available for these examplesMore in-depth focus on the areas of discrete and continuous state-space models and ecological statistics, including methods that have been specifically developed for each of these areasThis book is designed to make parameter redundancy and non-identifiability accessible and understandable to a wide audience from masters and PhD students to researchers, from mathematicians and statisticians to practitioners using mathematical or statistical models.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Cole, Diana},
	month = may,
	year = {2020},
}



@article{hodgesAre2010b,
	title = {Are exercises like this a good use of anybody's time?},
	volume = {91},
	doi = {10.2307/29779533},
	journal = {Ecology},
	author = {Hodges, James},
	month = dec,
	year = {2010},
	pages = {3496--500; discussion 3503},
}


@book{hodgesRichly2013,
	title = {Richly parameterized linear models: additive, time series, and spatial models using random effects},
	shorttitle = {Richly parameterized linear models},
	publisher = {CRC Press},
	author = {Hodges, James S.},
	year = {2013},
}



@article{woodStable2004,
	title = {Stable and {Efficient} {Multiple} {Smoothing} {Parameter} {Estimation} for {Generalized} {Additive} {Models}},
	volume = {99},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214504000000980},
	doi = {10.1198/016214504000000980},
	abstract = {Representation of generalized additive models (GAM's) using penalized regression splines allows GAM's to be employed in a straightforward manner using penalized regression methods. Not only is inference facilitated by this approach, but it is also possible to integrate model selection in the form of smoothing parameter selection into model fitting in a computationally efficient manner using well founded criteria such as generalized cross-validation. The current fitting and smoothing parameter selection methods for such models are usually effective, but do not provide the level of numerical stability to which users of linear regression packages, for example, are accustomed. In particular the existing methods cannot deal adequately with numerical rank deficiency of the GAM fitting problem, and it is not straightforward to produce methods that can do so, given that the degree of rank deficiency can be smoothing parameter dependent. In addition, models with the potential flexibility of GAM's can also present practical fitting difficulties as a result of indeterminacy in the model likelihood: Data with many zeros fitted by a model with a log link are a good example. In this article it is proposed that GAM's with a ridge penalty provide a practical solution in such circumstances, and a multiple smoothing parameter selection method suitable for use in the presence of such a penalty is developed. The method is based on the pivoted QR decomposition and the singular value decomposition, so that with or without a ridge penalty it has good error propagation properties and is capable of detecting and coping elegantly with numerical rank deficiency. The method also allows mixtures of user specified and estimated smoothing parameters and the setting of lower bounds on smoothing parameters. In terms of computational efficiency, the method compares well with existing methods. A simulation study compares the method to existing methods, including treating GAM's as mixed models.},
	number = {467},
	urldate = {2023-07-02},
	journal = {Journal of the American Statistical Association},
	author = {Wood, Simon N},
	month = sep,
	year = {2004},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214504000000980},
	keywords = {Penalized quasi-likelihood, Ridge regression, Generalized additive mixed model, REML, Generalized cross-validation, Regularization, Stable computation, Smoothing spline analysis of variance, Spline},
	pages = {673--686},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/RW2LNVWR/Wood - 2004 - Stable and Efficient Multiple Smoothing Parameter .pdf:application/pdf},
}


@article{bolkerStrategies2013a,
	title = {Strategies for fitting nonlinear ecological models in {R}, {AD} {Model} {Builder}, and {BUGS}},
	volume = {4},
	issn = {2041210X},
	url = {http://doi.wiley.com/10.1111/2041-210X.12044},
	doi = {10.1111/2041-210X.12044},
	number = {6},
	urldate = {2013-06-11},
	journal = {Methods in Ecology and Evolution},
	author = {Bolker, Benjamin M. and Gardner, Beth and Maunder, Mark and Berg, Casper W. and Brooks, Mollie and Comita, Liza and Crone, Elizabeth and Cubaynes, Sarah and Davies, Trevor and de Valpine, Perry and Ford, Jessica and Gimenez, Olivier and Kéry, Marc and Kim, Eun Jung and Lennert-Cody, Cleridy and Magnusson, Arni and Martell, Steve and Nash, John and Nielsen, Anders and Regetz, Jim and Skaug, Hans and Zipkin, Elise},
	month = jun,
	year = {2013},
	pages = {501--512},
	file = {onlineLibraryTPS.pdf:/home/bolker/Documents/zotero_new/storage/G2TAGJD5/onlineLibraryTPS.pdf:application/pdf},
}


@article{berahaJAGS2021,
	title = {{JAGS}, {NIMBLE}, {Stan}: a detailed comparison among {Bayesian} {MCMC} software},
	shorttitle = {{JAGS}, {NIMBLE}, {Stan}},
	url = {http://arxiv.org/abs/2107.09357},
	abstract = {The aim of this work is the comparison of the performance of the three popular software platforms JAGS, NIMBLE and Stan. These probabilistic programming languages are able to automatically generate samples from the posterior distribution of interest using MCMC algorithms, starting from the specification of a Bayesian model, i.e. the likelihood and the prior. The final goal is to present a detailed analysis of their strengths and weaknesses to statisticians or applied scientists. In this way, we wish to contribute to make them fully aware of the pros and cons of this software. We carry out a systematic comparison of the three platforms on a wide class of models, prior distributions, and data generating mechanisms. Our extensive simulation studies evaluate the quality of the MCMC chains produced, the efficiency of the software and the goodness of fit of the output. We also consider the efficiency of the parallelization made by the three platforms.},
	urldate = {2021-10-29},
	journal = {arXiv:2107.09357 [stat]},
	author = {Beraha, Mario and Falco, Daniele and Guglielmi, Alessandra},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.09357},
	keywords = {Statistics - Applications, Statistics - Computation},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/85ECY8N9/Beraha et al. - 2021 - JAGS, NIMBLE, Stan a detailed comparison among Ba.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/N4QIER4A/2107.html:text/html},
}


@article{michaudSequential2021,
	title = {Sequential {Monte} {Carlo} {Methods} in the nimble and {nimbleSMC} {R} {Packages}},
	volume = {100},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v100.i03},
	doi = {10.18637/jss.v100.i03},
	abstract = {nimble is an R package for constructing algorithms and conducting inference on hierarchical models. The nimble package provides a unique combination of flexible model specification and the ability to program model-generic algorithms. Specifically, the package allows users to code models in the BUGS language, and it allows users to write algorithms that can be applied to any appropriate model. In this paper, we introduce the nimbleSMC R package. nimbleSMC contains algorithms for state-space model analysis using sequential Monte Carlo (SMC) techniques that are built using nimble. We first provide an overview of state-space models and commonly-used SMC algorithms. We then describe how to build a state-space model in nimble and conduct inference using existing SMC algorithms within nimbleSMC. SMC algorithms within nimbleSMC currently include the bootstrap filter, auxiliary particle filter, ensemble Kalman filter, IF2 method of iterated filtering, and a particle Markov chain Monte Carlo (MCMC) sampler. These algorithms can be run in R or compiled into C++ for more efficient execution. Examples of applying SMC algorithms to linear autoregressive models and a stochastic volatility model are provided. Finally, we give an overview of how model-generic algorithms are coded within nimble by providing code for a simple SMC algorithm. This illustrates how users can easily extend nimble's SMC methods in high-level code.},
	language = {en},
	urldate = {2024-07-29},
	journal = {Journal of Statistical Software},
	author = {Michaud, Nicholas and de Valpine, Perry and Turek, Daniel and Paciorek, Christopher J. and Nguyen, Dao},
	month = nov,
	year = {2021},
	keywords = {auxiliary particle filter, ensemble Kalman filter, Liu and West filter, NIMBLE, particle filtering, particle MCMC, R, sequential Monte Carlo},
	pages = {1--39},
}

@article{devalpineProgramming2017,
	title = {Programming {With} {Models}: {Writing} {Statistical} {Algorithms} for {General} {Model} {Structures} {With} {NIMBLE}},
	volume = {26},
	issn = {1061-8600},
	shorttitle = {Programming {With} {Models}},
	url = {http://amstat.tandfonline.com/doi/abs/10.1080/10618600.2016.1172487},
	doi = {10.1080/10618600.2016.1172487},
	abstract = {We describe NIMBLE, a system for programming statistical algorithms for general model structures within R. NIMBLE is designed to meet three challenges: flexible model specification, a language for programming algorithms that can use different models, and a balance between high-level programmability and execution efficiency. For model specification, NIMBLE extends the BUGS language and creates model objects, which can manipulate variables, calculate log probability values, generate simulations, and query the relationships among variables. For algorithm programming, NIMBLE provides functions that operate with model objects using two stages of evaluation. The first stage allows specialization of a function to a particular model and/or nodes, such as creating a Metropolis-Hastings sampler for a particular block of nodes. The second stage allows repeated execution of computations using the results of the first stage. To achieve efficient second-stage computation, NIMBLE compiles models and functions via C++, using the Eigen library for linear algebra, and provides the user with an interface to compiled objects. The NIMBLE language represents a compilable domain-specific language (DSL) embedded within R. This article provides an overview of the design and rationale for NIMBLE along with illustrative examples including importance sampling, Markov chain Monte Carlo (MCMC) and Monte Carlo expectation maximization (MCEM). Supplementary materials for this article are available online.},
	number = {2},
	urldate = {2017-06-20},
	journal = {Journal of Computational and Graphical Statistics},
	author = {de Valpine, Perry and Turek, Daniel and Paciorek, Christopher J. and Anderson-Bergman, Clifford and Lang, Duncan Temple and Bodik, Rastislav},
	month = apr,
	year = {2017},
	pages = {403--413},
	file = {Snapshot:/home/bolker/Documents/zotero_new/storage/QVZ7A6TS/10618600.2016.html:text/html},
}


@article{goldinggreta2019,
	title = {greta: simple and scalable statistical modelling in {R}},
	volume = {4},
	issn = {2475-9066},
	shorttitle = {greta},
	url = {https://joss.theoj.org/papers/10.21105/joss.01601},
	doi = {10.21105/joss.01601},
	abstract = {Statistical modelling is used throughout the sciences. Often, statistical analyses require custom models that cannot be fitted using off-the shelf statistical software. These models can be specified in a statistical syntax and can then be automatically fit to data using methods such as Markov Chain monte Carlo (MCMC) and maximum likelihood. This lets users focus on the statistical nature of the model, rather than implementation details and inference procedures. Since the development of the widely successful WinBUGS (later developed as OpenBUGS; Spiegelhalter, Thomas, Best, \& Lunn (2014)) a number of alternative software packages for custom statistical modelling have been introduced, including JAGS, Stan, and NIMBLE (Carpenter et al., 2017; de Valpine et al., 2017; Plummer \& others, 2003). In these software packages, users typically write out models in a domain-specific language, which is then compiled into computational code. Though see the Python packages PyMC and Edward (Salvatier, Wiecki, \& Fonnesbeck, 2016; Tran et al., 2016) in which models are specified in Python code.},
	language = {en},
	number = {40},
	urldate = {2024-07-29},
	journal = {Journal of Open Source Software},
	author = {Golding, Nick},
	month = aug,
	year = {2019},
	pages = {1601},
	file = {Golding - 2019 - greta simple and scalable statistical modelling i.pdf:/home/bolker/Documents/zotero_new/storage/K9W6RQZH/Golding - 2019 - greta simple and scalable statistical modelling i.pdf:application/pdf},
}


@book{krainskiAdvanced2018,
	title = {Advanced {Spatial} {Modeling} with {Stochastic} {Partial} {Differential} {Equations} {Using} {R} and {INLA}},
	isbn = {978-0-429-62821-4},
	abstract = {Modeling spatial and spatio-temporal continuous processes is an important and challenging problem in spatial statistics. Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA describes in detail the stochastic partial differential equations (SPDE) approach for modeling continuous spatial processes with a Matérn covariance, which has been implemented using the integrated nested Laplace approximation (INLA) in the R-INLA package. Key concepts about modeling spatial processes and the SPDE approach are explained with examples using simulated data and real applications.  This book has been authored by leading experts in spatial statistics, including the main developers of the INLA and SPDE methodologies and the R-INLA package. It also includes a wide range of applications:  * Spatial and spatio-temporal models for continuous outcomes * Analysis of spatial and spatio-temporal point patterns * Coregionalization spatial and spatio-temporal models * Measurement error spatial models * Modeling preferential sampling * Spatial and spatio-temporal models with physical barriers * Survival analysis with spatial effects * Dynamic space-time regression * Spatial and spatio-temporal models for extremes * Hurdle models with spatial effects * Penalized Complexity priors for spatial models  All the examples in the book are fully reproducible. Further information about this book, as well as the R code and datasets used, is available from the book website at http://www.r-inla.org/spde-book.  The tools described in this book will be useful to researchers in many fields such as biostatistics, spatial statistics, environmental sciences, epidemiology, ecology and others. Graduate and Ph.D. students will also find this book and associated files a valuable resource to learn INLA and the SPDE approach for spatial modeling.},
	language = {en},
	publisher = {CRC Press},
	author = {Krainski, Elias T. and Gómez-Rubio, Virgilio and Bakka, Haakon and Lenzi, Amanda and Castro-Camilo, Daniela and Simpson, Daniel and Lindgren, Finn and Rue, Håvard},
	month = dec,
	year = {2018},
	keywords = {Mathematics / Probability \& Statistics / General},
}

@article{bakkaSpatial2018,
	title = {Spatial modeling with {R}-{INLA}: {A} review},
	volume = {10},
	copyright = {© 2018 Wiley Periodicals, Inc.},
	issn = {1939-0068},
	shorttitle = {Spatial modeling with {R}-{INLA}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1443},
	doi = {https://doi.org/10.1002/wics.1443},
	abstract = {Coming up with Bayesian models for spatial data is easy, but performing inference with them can be challenging. Writing fast inference code for a complex spatial model with realistically-sized datasets from scratch is time-consuming, and if changes are made to the model, there is little guarantee that the code performs well. The key advantages of R-INLA are the ease with which complex models can be created and modified, without the need to write complex code, and the speed at which inference can be done even for spatial problems with hundreds of thousands of observations. R-INLA handles latent Gaussian models, where fixed effects, structured and unstructured Gaussian random effects are combined linearly in a linear predictor, and the elements of the linear predictor are observed through one or more likelihoods. The structured random effects can be both standard areal model such as the Besag and the BYM models, and geostatistical models from a subset of the Matérn Gaussian random fields. In this review, we discuss the large success of spatial modeling with R-INLA and the types of spatial models that can be fitted, we give an overview of recent developments for areal models, and we give an overview of the stochastic partial differential equation (SPDE) approach and some of the ways it can be extended beyond the assumptions of isotropy and separability. In particular, we describe how slight changes to the SPDE approach leads to straight-forward approaches for nonstationary spatial models and nonseparable space–time models. This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Bayesian Methods and Theory Statistical Models {\textgreater} Bayesian Models Data: Types and Structure {\textgreater} Massive Data},
	language = {en},
	number = {6},
	urldate = {2021-04-17},
	journal = {WIREs Computational Statistics},
	author = {Bakka, Haakon and Rue, Håvard and Fuglstad, Geir-Arne and Riebler, Andrea and Bolin, David and Illian, Janine and Krainski, Elias and Simpson, Daniel and Lindgren, Finn},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1443},
	keywords = {spatial statistics, approximate Bayesian inference, Gaussian Markov random fields, Laplace approximations, sparse matrices, stochastic partial differential equations},
	pages = {e1443},
	file = {Accepted Version:/home/bolker/Documents/zotero_new/storage/YV8G7RJW/Bakka et al. - 2018 - Spatial modeling with R-INLA A review.pdf:application/pdf;Snapshot:/home/bolker/Documents/zotero_new/storage/3MC66G6P/wics.html:text/html},
}

@article{berildImportance2021,
	title = {Importance {Sampling} with the {Integrated} {Nested} {Laplace} {Approximation}},
	url = {http://arxiv.org/abs/2103.02721},
	abstract = {The Integrated Nested Laplace Approximation (INLA) is a deterministic approach to Bayesian inference on latent Gaussian models (LGMs) and focuses on fast and accurate approximation of posterior marginals for the parameters in the models. Recently, methods have been developed to extend this class of models to those that can be expressed as conditional LGMs by fixing some of the parameters in the models to descriptive values. These methods differ in the manner descriptive values are chosen. This paper proposes to combine importance sampling with INLA (IS-INLA), and extends this approach with the more robust adaptive multiple importance sampling algorithm combined with INLA (AMIS-INLA). This paper gives a comparison between these approaches and existing methods on a series of applications with simulated and observed datasets and evaluates their performance based on accuracy, efficiency, and robustness. The approaches are validated by exact posteriors in a simple bivariate linear model; then, they are applied to a Bayesian lasso model, a Bayesian imputation of missing covariate values, and lastly, in parametric Bayesian quantile regression. The applications show that the AMIS-INLA approach, in general, outperforms the other methods, but the IS-INLA algorithm could be considered for faster inference when good proposals are available.},
	urldate = {2022-01-21},
	journal = {arXiv:2103.02721 [stat]},
	author = {Berild, Martin Outzen and Martino, Sara and Gómez-Rubio, Virgilio and Rue, Håvard},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.02721},
	keywords = {Statistics - Methodology, Statistics - Computation},
	file = {arXiv Fulltext PDF:/home/bolker/Documents/zotero_new/storage/X7H57U7D/Berild et al. - 2021 - Importance Sampling with the Integrated Nested Lap.pdf:application/pdf;arXiv.org Snapshot:/home/bolker/Documents/zotero_new/storage/PWH5M4RK/2103.html:text/html},
}

@book{soyerDynamic2022,
	address = {New York},
	title = {Dynamic {Time} {Series} {Models} using {R}-{INLA}: {An} {Applied} {Perspective}},
	isbn = {978-1-00-313403-9},
	shorttitle = {Dynamic {Time} {Series} {Models} using {R}-{INLA}},
	abstract = {Dynamic Time Series Models using R-INLA: An Applied Perspective is the outcome of a joint effort to systematically describe the use of R-INLA for analysing time series and showcasing the code and description by several examples. This book introduces the underpinnings of R-INLA and the tools needed for modelling different types of time series using an approximate Bayesian framework.
The book is an ideal reference for statisticians and scientists who work with time series data. It provides an excellent resource for teaching a course on Bayesian analysis using state space models for time series.
Key Features:

Introduction and overview of R-INLA for time series analysis.
Gaussian and non-Gaussian state space models for time series.
State space models for time series with exogenous predictors.
Hierarchical models for a potentially large set of time series. 
Dynamic modelling of stochastic volatility and spatio-temporal dependence.},
	publisher = {Chapman and Hall/CRC},
	author = {Soyer, Balaji Raman, Refik, Nalini Ravishanker},
	month = aug,
	year = {2022},
	doi = {10.1201/9781003134039},
	file = {Full Text PDF:/home/bolker/Documents/zotero_new/storage/27ADFJ4E/Soyer - 2022 - Dynamic Time Series Models using R-INLA An Applie.pdf:application/pdf},
}



@book{krainskiAdvanced2018,
	title = {Advanced {Spatial} {Modeling} with {Stochastic} {Partial} {Differential} {Equations} {Using} {R} and {INLA}},
	isbn = {978-0-429-62821-4},
	abstract = {Modeling spatial and spatio-temporal continuous processes is an important and challenging problem in spatial statistics. Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA describes in detail the stochastic partial differential equations (SPDE) approach for modeling continuous spatial processes with a Matérn covariance, which has been implemented using the integrated nested Laplace approximation (INLA) in the R-INLA package. Key concepts about modeling spatial processes and the SPDE approach are explained with examples using simulated data and real applications.  This book has been authored by leading experts in spatial statistics, including the main developers of the INLA and SPDE methodologies and the R-INLA package. It also includes a wide range of applications:  * Spatial and spatio-temporal models for continuous outcomes * Analysis of spatial and spatio-temporal point patterns * Coregionalization spatial and spatio-temporal models * Measurement error spatial models * Modeling preferential sampling * Spatial and spatio-temporal models with physical barriers * Survival analysis with spatial effects * Dynamic space-time regression * Spatial and spatio-temporal models for extremes * Hurdle models with spatial effects * Penalized Complexity priors for spatial models  All the examples in the book are fully reproducible. Further information about this book, as well as the R code and datasets used, is available from the book website at http://www.r-inla.org/spde-book.  The tools described in this book will be useful to researchers in many fields such as biostatistics, spatial statistics, environmental sciences, epidemiology, ecology and others. Graduate and Ph.D. students will also find this book and associated files a valuable resource to learn INLA and the SPDE approach for spatial modeling.},
	language = {en},
	publisher = {CRC Press},
	author = {Krainski, Elias T. and Gómez-Rubio, Virgilio and Bakka, Haakon and Lenzi, Amanda and Castro-Camilo, Daniela and Simpson, Daniel and Lindgren, Finn and Rue, Håvard},
	month = dec,
	year = {2018},
	keywords = {Mathematics / Probability \& Statistics / General},
}


@book{lunnBUGS2012a,
	address = {Boca Raton, FL},
	edition = {1st edition},
	title = {The {BUGS} {Book}},
	isbn = {978-1-58488-849-9},
	abstract = {Bayesian statistical methods have become widely used for data analysis and modelling in recent years, and the BUGS software has become the most popular software for Bayesian analysis worldwide. Authored by the team that originally developed this software, The BUGS Book provides a practical introduction to this program and its use. The text presents complete coverage of all the functionalities of BUGS, including prediction, missing data, model criticism, and prior sensitivity. It also features a large number of worked examples and a wide range of applications from various disciplines.The book introduces regression models, techniques for criticism and comparison, and a wide range of modelling issues before going into the vital area of hierarchical models, one of the most common applications of Bayesian methods. It deals with essentials of modelling without getting bogged down in complexity. The book emphasises model criticism, model comparison, sensitivity analysis to alternative priors, and thoughtful choice of prior distributions―all those aspects of the "art" of modelling that are easily overlooked in more theoretical expositions. More pragmatic than ideological, the authors systematically work through the large range of "tricks" that reveal the real power of the BUGS software, for example, dealing with missing data, censoring, grouped data, prediction, ranking, parameter constraints, and so on. Many of the examples are biostatistical, but they do not require domain knowledge and are generalisable to a wide range of other application areas. Full code and data for examples, exercises, and some solutions can be found on the book’s website.},
	language = {English},
	publisher = {Routledge},
	author = {Lunn, David},
	month = nov,
	year = {2012},
}


@article{fournierAD2011,
	title = {{AD} {Model} {Builder}: using automatic differentiation for statistical inference of highly parameterized complex nonlinear models},
	issn = {1055-6788},
	shorttitle = {{AD} {Model} {Builder}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10556788.2011.597854},
	doi = {10.1080/10556788.2011.597854},
	abstract = {Many criteria for statistical parameter estimation, such as maximum likelihood, are formulated as a nonlinear optimization problem. Automatic Differentiation Model Builder (ADMB) is a programming framework based on automatic differentiation, aimed at highly nonlinear models with a large number of parameters. The benefits of using AD are computational efficiency and high numerical accuracy, both crucial in many practical problems. We describe the basic components and the underlying philosophy of ADMB, with an emphasis on functionality found in no other statistical software. One example of such a feature is the generic implementation of Laplace approximation of high-dimensional integrals for use in latent variable models. We also review the literature in which ADMB has been used, and discuss future development of ADMB as an open source project. Overall, the main advantages of ADMB are flexibility, speed, precision, stability and built-in methods to quantify uncertainty.
Many criteria for statistical parameter estimation, such as maximum likelihood, are formulated as a nonlinear optimization problem. Automatic Differentiation Model Builder (ADMB) is a programming framework based on automatic differentiation, aimed at highly nonlinear models with a large number of parameters. The benefits of using AD are computational efficiency and high numerical accuracy, both crucial in many practical problems. We describe the basic components and the underlying philosophy of ADMB, with an emphasis on functionality found in no other statistical software. One example of such a feature is the generic implementation of Laplace approximation of high-dimensional integrals for use in latent variable models. We also review the literature in which ADMB has been used, and discuss future development of ADMB as an open source project. Overall, the main advantages of ADMB are flexibility, speed, precision, stability and built-in methods to quantify uncertainty.},
	journal = {Optimization Methods and Software},
	author = {Fournier, David   A. and Skaug, Hans   J. and Ancheta, Johnoel and Ianelli, James and Magnusson, Arni and Maunder, Mark   N. and Nielsen, Anders and Sibert, John},
	year = {2011},
	pages = {1--17},
	file = {T&F PDF fulltext:/home/bolker/Documents/zotero_new/storage/896B46TK/Fournier et al. - 2011 - AD Model Builder using automatic differentiation .pdf:application/pdf;T&F Snapshot:/home/bolker/Documents/zotero_new/storage/FAW2A362/10556788.2011.html:text/html},
}
