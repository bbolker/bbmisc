---
title: "Improving statistical computation in R"
bibliography: rcomp.bib
author: Ben Bolker
date: today
date-format: iso
format: 
  revealjs:
     slide-number: true
     show-slide-number: all
     template-partials:
      - title-slide.html
---

```{r setup, include=FALSE}
library(tidyverse)
library(colorspace)
library(see)
library(Rcpp)
theme_set(theme_bw(base_size=16))
```

# introduction

## should you optimize?

* "premature optimization is the root of all evil" (Hoare/Knuth): "software engineers should worry about other issues (such as good algorithm design and good implementations of those algorithms) before they worry about micro-optimizations" [@hydeFallacy2009]
* @rossFasteR2013:

> Do you have other sh*t to do? No? Please contact me and Iâ€™ll help you with that. Yes? You are among the 90% of R users whose first priority is not computer programming. The time spent optimizing code is often longer than the computing time actually saved. Use the simple solutions and you can get on with your research/life.

## categories of optimization

* low-hanging fruit
* parallelization
* memoization
* automatic differentiation
* faster languages

# benchmarking and profiling

## measuring code performance

* *measure* speed before you try to fix it
* complexity theory/[big-O notation](https://en.wikipedia.org/wiki/Big_O_notation) (e.g., $f(x) = {\cal O}(x^{3/2})$ is useful *if*
   * you have some idea of the scaling constant
   * you plan to scale to large problems (or someone does)
* scaling dimensions: number of observations, parameters, clusters, dimensions ...
* [Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law): speeding up *bottlenecks* is what matters

## benchmarking

* examples should be large enough not to be swamped by overhead
* ... but small enough to run replicates and average
* `system.time()` for quick estimates
* `rbenchmark`, `microbenchmark` packages 
* estimating scaling: theoretical or empirical (log-log plot)

## scaling example

Modified from @brooksGlmmTMB2017:

```{r contr_scale, echo = FALSE}
load("contraceptionTimings.rda")
op <- options(warn = -1)
gg0 <- ggplot(tmatContraception,
       aes(n, time, colour=pkg)) + geom_point() +
    scale_y_log10(breaks=c(1,2,5,10,20,50,100)) +
    scale_x_log10(breaks=c(1,2,4,10,20,40)) +
    labs(x="Replication (x 1934 obs.)",y="Elapsed time (s)") +
    geom_smooth(method="lm", formula = y ~ x) +
    see::scale_color_okabeito()
gg0 + geom_function(fun = \(x) 5*x, linetype = 2, colour = "black")
```

## profiling

From @ihakaWriting2009:

```{r rw2d2, echo=TRUE}
rw2d2 <- function(n) {
    steps <- sample(c(-1, 1), n - 1, replace = TRUE)
    xdir <- sample(c(TRUE, FALSE), n - 1, replace = TRUE)
    xpos <- c(0, cumsum(ifelse(xdir, steps, 0)))
    ypos <- c(0, cumsum(ifelse(xdir, 0, steps)))
    return(list(x = xpos, y = ypos))
}
```

```{r profrun, echo = FALSE}
if (!file.exists("Rprof.out")) {
    ## better to do this *outside*, call stack looks better
    Rprof()
    for (i in 1:100) {
        pos <- rw2d2(1e5)
    }
    Rprof(NULL)
}
```

## profiling run

* run multiple times to collect enough data/average over variation

```{r profrun_fake, eval = FALSE, echo = TRUE}
Rprof("Rprof.out") ## start profiling
for (i in 1:100) {
    pos <- rw2d2(1e5)
}
Rprof(NULL) ## stop profiling
```

## profiling results

```{r show_prof, echo = TRUE}
source("https://raw.githubusercontent.com/noamross/noamtools/master/R/proftable.R")
proftable("Rprof.out", lines = 5)
```

## profiling

* see also `summaryRprof()` (base-R), `profvis` package (RStudio fanciness)
* easiest to interpret if code is organized into functions

# low-hanging fruit

## avoid writing 'bad' R code

:::: {.columns}

::: {.column width="60%"}
* avoid growing objects (chapter 2)
   * pre-allocate (`numeric()` etc.)
   * create lists and `rbind()` them together
* failing to vectorize (chapter 3)
* over-vectorizing (chapter 4)
:::

::: {.column width="40%"}
@burnsInferno2012

![](pix/inferno.jpg)

 "If you are using R and you think you're in hell, this is a map for you."
:::

::::

## use good packages

* especially for data handling, I/O
   * `data.table` > `tidyverse` > base R
   * data formats: `arrow`, `vroom`, direct database access (`dbplyr`)
* `collapse`, `xts`, `Rfast`

# parallelization

## parallelization types

* distributed: "embarrassingly parallel"
* multicore: multiple R processes
   * each one copies all objects
* parallel vs "embarrassingly parallel"/distributed
* threading: lightweight, shared-memory 
* see [parallel and HPC task view](https://cran.r-project.org/web/views/HighPerformanceComputing.html)

## distributed computing

* start individual runs (e.g. simulations, sets of simulations) as completely separate R jobs
* Compute Canada (*or* AWS *or* a big local workstation)
   * [notes on R and distributed computing on SHARCnet](https://hackmd.io/K3IF612hT8e5r4vtervkIA?view)
* jobs assigned by batch scheduler: High Performance Computing
* some tools within R: `slurm`, `batchtools`, `futures.batchtools` package

## multicore

* spawn multiple jobs on a single machine (multiple cores)
* `parallel` package, `foreach`/`doParallel`

## threading

* not available directly through R
* `OpenMP`
* parallel BLAS:  https://csantill.github.io/RPerformanceWBLAS/
  * some amount of *command-line bullshittery* required [@browneCommandline2021]

# automatic differentiation

* @griewankIntroduction2003
* @chauAutomatic2022
* RTMB, TMB
* @keydanaDeep2023; https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/

## AD example

e.g. `cos(log(1+x^2))`

```{r deriv}
deriv(~cos(log(1+x^2)), "x")
```

## better AD in R?

* `Deriv` package

# memoization

## memoization

* automatically store previously calculated values

```{r}
myfac <- function(n) {
    if (n==0) return(1)
    return(n*myfac(n-1))
}
library(memoise)
myfac <- memoise(myfac)
```

```{r}
microbenchmark::microbenchmark(myfac(20), myfac2(20))
# integration with lower-level languages

https://csgillespie.github.io/efficientR/performance.html

# general tips


<!-- https://quarto.org/docs/presentations/revealjs/presenting.html -->

<!-- CC badge in title:
https://quarto.org/docs/journals/templates.html#template-partials
https://github.com/quarto-dev/quarto-cli/blob/main/src/resources/formats/revealjs/pandoc/title-slide.html -->

## bad

```{r}
piR_slow <- function(N) {
    res <- numeric(0)
    for (i in 1:N) {
        res <- c(res, as.numeric(runif(1)^2 + runif(1)^2 < 1.0))
    }
    4*sum(res)/N
}
```

## better

* vectorize, don't grow objects

```{r}
piR <- function(N) {
    x <- runif(N)
    y <- runif(N)
    d <- sqrt(x^2 + y^2)
    return(4 * sum(d < 1.0) / N)
}
```

## rcpp

```{r}
pi_rcpp <- cppFunction("
double piSugar(const int N) {
    NumericVector x = runif(N);
    NumericVector y = runif(N);
    NumericVector d = sqrt(x*x + y*y);
    return 4.0 * sum(d < 1.0) / N;
}")
```

```{r cache=TRUE}
rbenchmark::benchmark(piR_slow(1e4), piR(1e5), pi_rcpp(1e5))
```

## to do

* reorganize!
* different glmmTMB-scaling pic?
* command line bullshittery
* RcppEigen, RcppArmadillo

## References
