---
title: "RTMB intro"
bibliography: rtmb.bib
---

## what is RTMB?

* like TMB [@kristensenTMB2016] but more convenient (although less mature)
* evaluation of R functions in compiled code (sort of)
* automatic differentiation
* Laplace approximation engine

![](pix/TMB.png)

```{r pkgs, message=FALSE}
library(bbmle)
library(RTMB)
library(tmbstan)
library(igraph)
library(ggplot2); theme_set(theme_bw())
data("prussian", package = "pscl")
```

## what is autodiff?

- fancy chain rule
- various ways to do it
- $C(\ell'_\theta)<4C(\ell)$ ("cheap gradient principle")
- time-efficient, maybe memory-hungry

## simple (?) example

```{r}
f1 <- function(param) {
    param$x^2 + cos(param$y)
}
par1 <- list(x=1, y = 2)
lobstr::ast(!!body(f1))
```

```{r}
ff1 <- MakeADFun(f1, par1)
names(ff1)
ff1$fn()  ## uses $par (== par1) as default
tape1 <- MakeTape(f1, par1)
plot(graph_from_adjacency_matrix(tape1$graph()),
     layout=layout_as_tree, vertex.size = 25)
```

No speed advantage yet ...

```{r}
bench1 <- microbenchmark::microbenchmark(ff1$fn(), f1(par1))
print(bench1)
```

But we can get gradients right away.

```{r}
ff1$gr()
```

## a GLM

```{r}
X <- model.matrix(~ factor(year), data = prussian)
par2 <- list(beta = rep(0, ncol(X)))
f2 <- function(pars) {
    getAll(pars)
    mu <- exp(X %*% beta)
    -sum(dpois(prussian$y, lambda = mu, log = TRUE))
}
f2(par2)
ff2 <- MakeADFun(f2, par2)
ff2$fn()
```

```{r}
bench2 <- microbenchmark::microbenchmark(ff2$fn(), f2(par2))
print(bench2)
```

```{r}
fit2 <- with(ff2, nlminb(par, fn, gr))
```

```{r}
g2 <- glm(y ~ factor(year), data = prussian, family = poisson)
all.equal(unname(coef(g2)), unname(fit2$par), tolerance = 1e-6)
```

## GLMM

We can make this a GLMM with *very* little additional effort.

```{r}
Z_y <- Matrix::sparse.model.matrix(~year, data = prussian)
Z_c <- Matrix::sparse.model.matrix(~corp, data = prussian)
par3 <- list(beta0 = 0, b_y = rep(0, ncol(Z_y)), b_c = rep(0, ncol(Z_c)),
             logsd_c = 0, logsd_y = 0)
f3 <- function(pars) {
    getAll(pars)
    mu <- exp(beta0 + Z_y %*% b_y + Z_c %*% b_c)
    mu <- drop(as.matrix(mu))
    nll <- -sum(dpois(prussian$y, lambda = mu, log = TRUE))
    nll <- nll - sum(dnorm(b_y, sd = exp(logsd_y), log = TRUE))
    nll <- nll - sum(dnorm(b_c, sd = exp(logsd_c), log = TRUE))
    return(nll)
}
f3(par3)
ff3 <- MakeADFun(f3, par3)
ff3$fn()
ff3 <- MakeADFun(f3, par3, random = c("b_y", "b_c"), silent = TRUE)
ff3$fn()
fit3 <- with(ff3, nlminb(par, fn, gr))
fit3$par
```

## more complexity (beta-binomial)

```{r}
library(bbmle)
library(emdbook)
load(system.file("vignetteData","orob1.rda",package="bbmle"))
m1 <- mle2(m~dbetabinom(prob,size=n,theta),
            param=list(prob~dilution),
            start=list(prob=0.5,theta=1),
            data=orob1)
```

```{r}
bb <- function (x, prob, size, theta, log = FALSE)  {
    v <- lfactorial(size) - lfactorial(x) - lfactorial(size - x) -
        lbeta(theta * (1 - prob), theta * prob) +
        lbeta(size - x + theta * (1 - prob), x + theta * prob)
    if (log) v else exp(v)
}
```

```{r}
X <- model.matrix(~dilution, data = orob1)
tmbdata <- list(n = orob1$n, m = orob1$m, X = X)
lbeta <- function(a, b) lgamma(a) + lgamma(b) - lgamma(a+b)

bb <- function (x, prob, size, theta, shape1, shape2, log = FALSE) 
{
    if (missing(prob) && !missing(shape1) && !missing(shape2)) {
        prob <- shape1/(shape1 + shape2)
        theta <- shape1 + shape2
    }
    v <- lfactorial(size) - lfactorial(x) -
        lfactorial(size - x) - lbeta(theta * (1 - prob), theta * prob) +
        lbeta(size - x + theta * (1 - prob), x + theta * prob)
    if (log) v else exp(v)
}

fn <- function(param) {
    getAll(param, tmbdata)
    probvec <- plogis(X %*% beta_prob)
    thetavec <- exp(X %*% beta_theta)
    -sum(bb(m, prob = probvec,
                             size = n,
                             theta = thetavec, log = TRUE))
}
pars <- list(beta_prob = rep(0, 3), beta_theta = rep(0,3))
fn(pars)    

ff <- MakeADFun(fn, pars)
ff$fn()
autoplot(microbenchmark::microbenchmark(fn(pars), ff$fn()),
         times = 1000)
```

```{r echo =FALSE, eval = FALSE}
## digression: testing von Mises definition
## taken from circular:::DvonmisesRad, without worrying about non-log scale
##  or limiting values of mu=0 etc)
library(circular)
dvm <- function(pars) {
    getAll(pars)
    kappa <- exp(log_kappa)
    sum(log(2 * pi) + log(besselI(kappa, nu = 0) + kappa) +
        kappa * (cos(x - 2*pi*plogis(logit_mu))))
}
x <- rvonmises(1000, mu = pi/4, kappa = 2)
pars <- list(log_kappa=0, logit_mu = 0)
dvm(pars)
ff <- MakeADFun(dvm, pars)
ff$fn(pars)
```

## tricks and traps

* have to handle prediction, tests, diagnostics, etc. etc. yourself
* data handling (see [here](https://groups.google.com/g/tmb-users/c/sq3y5aTwvjo), [here](https://groups.google.com/g/tmb-users/c/YzSjsHyFYJ8)
* non-standard probability distributions
* use of `<-[` (see [here](https://groups.google.com/g/tmb-users/c/HlPqkfcCa1g)) etc.

## documentation and links

- http://www.nielsensweb.org/swansea2024/
