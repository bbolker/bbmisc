---
title: "RTMB intro"
bibliography: rtmb.bib
author: "Ben Bolker"
date: today
licence: CC-BY-SA
code-annotations: below
format:
  html:
    embed-resources: true
---

## what is RTMB?

* like TMB [@kristensenTMB2016] but more convenient (although less mature)
* evaluation of R functions in compiled code (sort of)
* automatic differentiation
* Laplace approximation engine

![](pix/TMB.png)

```{r pkgs, message=FALSE}
library(bbmle)
library(RTMB)
library(tmbstan)
library(igraph)
library(ggplot2); theme_set(theme_bw())
data("prussian", package = "pscl")
```

## what is autodiff?

- fancy chain rule
- various ways to do it
- $C(\ell'_\theta)<4C(\ell)$ ("cheap gradient principle")
- time-efficient, maybe memory-hungry

## simple (?) example

```{r ex1-ast}
f1 <- function(param) {
    param$x^2 + cos(param$y)
}
par1 <- list(x=1, y = 2)
## lobstr::ast(!!body(f1))
```

```{r ex1-tape}
tape1 <- MakeTape(f1, par1)
plot(graph_from_adjacency_matrix(tape1$graph()),
     layout=layout_as_tree, vertex.size = 25)
```

Now we construct the full TMB object (includes objective function, gradient, etc.)
```{r ex1-makefn}
ff1 <- MakeADFun(f1, par1)
names(ff1)
ff1$fn()  ## uses $par (== par1) as default
```

No speed advantage yet ...

```{r ex1-bench}
bench1 <- microbenchmark::microbenchmark(ff1$fn(), f1(par1))
print(bench1)
```

But we can get gradients right away.

```{r ex1-gr}
ff1$gr()
```

## a simple GLM

Set up model matrix (can also parameterize the model 'by hand', i.e. `beta0 + beta_y[year]` etc., but I find that extremely tedious).

```{r ex2}
X2 <- model.matrix(~ factor(year), data = prussian)
par2 <- list(beta = rep(0, ncol(X2)))
f2 <- function(pars) {
    getAll(pars)  ## this is like with() or attach()
    mu <- exp(X2 %*% beta)
    -sum(dpois(prussian$y, lambda = mu, log = TRUE))
}
f2(par2)
ff2 <- MakeADFun(f2, par2, silent = TRUE)
ff2$fn()
ff2$gr()
```

```{r ex2-bench}
bench2 <- microbenchmark::microbenchmark(ff2$fn(), f2(par2))
print(bench2)
```

Objective function in RTMB is already twice as fast ...

```{r fit2}
fit2 <- with(ff2, nlminb(par, fn, gr))
```

Compare with GLM fit:

```{r glm2}
g2 <- glm(y ~ factor(year), data = prussian, family = poisson)
all.equal(unname(coef(g2)), unname(fit2$par), tolerance = 1e-6)
```

## GLMM

We can make this a GLMM with *very* little additional effort. Again, we can parameterize either 'by hand' or with a random effects model matrix $Z$ (see [here](http://bbolker.github.io/bbmisc/rtmb_glmm.html) for a more complex but more general formulation of GLMMs in RTMB ...)

```{r ex3}
## RTMB knows about sparse matrices
Z_y <- Matrix::sparse.model.matrix(~year, data = prussian)
Z_c <- Matrix::sparse.model.matrix(~corp, data = prussian)
par3 <- list(beta0 = 0, b_y = rep(0, ncol(Z_y)), b_c = rep(0, ncol(Z_c)),
             logsd_c = 0, logsd_y = 0)
f3 <- function(pars) {
    getAll(pars)
    mu <- exp(beta0 + Z_y %*% b_y + Z_c %*% b_c)
    ## need to convert to a vector here ...
    mu <- drop(as.matrix(mu))
    ## conditional (negative) log-likelihood
    nll <- -sum(dpois(prussian$y, lambda = mu, log = TRUE))
    ## log-likelihoods of the random effects
    nll <- nll - sum(dnorm(b_y, sd = exp(logsd_y), log = TRUE))
    nll <- nll - sum(dnorm(b_c, sd = exp(logsd_c), log = TRUE))
    return(nll)
}
f3(par3)
## make AD fun *without* Laplace approximation
ff3 <- MakeADFun(f3, par3, silent = TRUE)
ff3$fn()
## now integrate over random effects
ff3 <- MakeADFun(f3, par3, random = c("b_y", "b_c"), silent = TRUE)
ff3$fn()
fit3 <- with(ff3, nlminb(par, fn, gr))
fit3$par
```

## tricks and traps

* objective function **must be differentiable** with respect to parameters (no `if()`, `abs()`, `round()`, `min()`, `max()` depending on parameters)
* have to handle prediction, tests, diagnostics, etc. etc. yourself
* data handling (see [here](https://groups.google.com/g/tmb-users/c/sq3y5aTwvjo), [here](https://groups.google.com/g/tmb-users/c/YzSjsHyFYJ8)) (and very similar arguments from 2004 about [MLE fitting machinery taking a `data` argument](https://hypatia.math.ethz.ch/pipermail/r-devel/2004-June/029837.html)
   * if you do something clever where you define your objective function in a different environment from where you call `MakeADFun`, you can use `assign(..., environment(objective_function))` to make sure that the objective function can see any objects it needs to know about ...
* have to implement exotic probability distributions yourself
* use of `<-[` (see [here](https://groups.google.com/g/tmb-users/c/HlPqkfcCa1g)) etc.
   * specifically, if you use the `c()` function, or if you use the `diag<-` function (which sets the diagonal of a matrix) or the `[<-` function (which assigns values within a matrix), you need to add e.g. `ADoverload("[<-")` to the beginning of your function
* for matrix exponentials, you should use `Matrix::expm()` rather than `expm::expm()`
* RTMB is pickier than R about matrices. You may need to use some combination of `drop()` and `as.matrix()` to convert matrices with dimension 1 in some direction (or `Matrix` matrices) back to vectors
* not widely used yet; there will be holes
* compare to: `bbmle` (no random effects, no autodiff); [NIMBLE](https://r-nimble.org/) (more restricted set of back-ends); Stan (see [tmbstan](https://CRAN.R-project.org/package=tmbstan)), JAGS, etc..
* `[[`-indexing may be much faster than `[`-indexing: see [here](https://groups.google.com/g/tmb-users/c/rm2N5mH8U-8/m/l1sYZov3EAAJ) (and later messages in that thread)
* if you use `cat()` or `print()` to print out numeric values, the results may not make sense (you'll see a printout of RTMB's internal representation of autodiff-augmented numbers ...)

## if transitioning from TMB

* RTMB uses `%*%` (as in base R), not `*` (as in C++) for matrix/matrix and matrix/vector multiplication


## documentation and links

- [RTMB on R-universe](https://kaskr.r-universe.dev/RTMB)
- [ISEC 2024 workshop](http://www.nielsensweb.org/swansea2024/)
- [TMB-users list](https://groups.google.com/g/tmb-users) on Google Groups
- [GLMMs in RTMB](http://bbolker.github.io/bbmisc/rtmb_glmm.html)

## more complexity (beta-binomial)

```{r betabinom-mle2, message=FALSE, warning=FALSE}
library(bbmle)
library(emdbook)
load(system.file("vignetteData","orob1.rda",package="bbmle"))
m1 <- mle2(m~dbetabinom(prob,size=n,theta),
            param=list(prob~dilution),
            start=list(prob=0.5,theta=1),
            data=orob1)
```

(this produces lots of `lbeta` `NaN` warnings)

```{r}
bb <- function (x, prob, size, theta, log = FALSE)  {
    v <- lfactorial(size) - lfactorial(x) - lfactorial(size - x) -
        lbeta(theta * (1 - prob), theta * prob) +
        lbeta(size - x + theta * (1 - prob), x + theta * prob)
    if (log) v else exp(v)
}
```

```{r}
X <- model.matrix(~dilution, data = orob1)
tmbdata <- list(n = orob1$n, m = orob1$m, X = X)
lbeta <- function(a, b) lgamma(a) + lgamma(b) - lgamma(a+b)

bb <- function (x, prob, size, theta, shape1, shape2, log = FALSE) 
{
    if (missing(prob) && !missing(shape1) && !missing(shape2)) {
        prob <- shape1/(shape1 + shape2)
        theta <- shape1 + shape2
    }
    v <- lfactorial(size) - lfactorial(x) -
        lfactorial(size - x) - lbeta(theta * (1 - prob), theta * prob) +
        lbeta(size - x + theta * (1 - prob), x + theta * prob)
    if (log) v else exp(v)
}

fn <- function(param) {
    getAll(param, tmbdata)
    probvec <- plogis(X %*% beta_prob)
    thetavec <- exp(X %*% beta_theta)
    -sum(bb(m, prob = probvec,
                             size = n,
                             theta = thetavec, log = TRUE))
}
pars <- list(beta_prob = rep(0, 3), beta_theta = rep(0,3))
fn(pars)    

ff <- MakeADFun(fn, pars)
ff$fn()
autoplot(microbenchmark::microbenchmark(fn(pars), ff$fn()),
         times = 1000) + aes(fill = I("gray"))
```

```{r echo =FALSE, eval = FALSE}
## digression: testing von Mises definition
## taken from circular:::DvonmisesRad, without worrying about non-log scale
##  or limiting values of mu=0 etc)
library(circular)
dvm <- function(pars) {
    getAll(pars)
    kappa <- exp(log_kappa)
    sum(log(2 * pi) + log(besselI(kappa, nu = 0) + kappa) +
        kappa * (cos(x - 2*pi*plogis(logit_mu))))
}
x <- rvonmises(1000, mu = pi/4, kappa = 2)
pars <- list(log_kappa=0, logit_mu = 0)
dvm(pars)
ff <- MakeADFun(dvm, pars)
ff$fn(pars)
```

