---
title: disaggregating clarity of statistical results
date: today
author: Ben Bolker and Jonathan Dushoff
format:
  html:
    embed-resources: true
    code-fold: true
bibliography: sesoi.bib    
---

(Source code is in [this directory](https://github.com/bbolker/bbmisc/tree/master/Rmisc))

```{r setup, message = FALSE}
library(tidyverse)
library(ggplot2); theme_set(theme_bw() + theme(panel.spacing=grid::unit(0, "lines")))
library(directlabels)


## Okabe-Ito minus black and yellow
oi3 <- palette.colors(9)[-c(1, 5)]
out_scale <- scale_colour_manual(name = "outcome category",  values = oi3)
source("sim_sesoi_funs.R")
```

Simulate a $t$-test-like comparison: what is the distribution of outcomes?

We are classifying outcomes in the following way (from [here](https://dushoff.github.io/ResearchSandbox/clarStrength.Rout.pdf)/[here](https://github.com/dushoff/ResearchSandbox/blob/main/clarStrength.R):

```{r dushoff_effects}
largeEffect=1.3; smallEffect=0.5; tinyEffect = 0.1; smallVar = 0.2; medVar = 0.4; largeVar = 0.7; hugeVar = 1.2
span = 2
vget = Vectorize(get)
vr <- (
	read.table(header=TRUE, strip.white=TRUE, sep=":", text="
		pic : val : unc : atext : ntext
		PL : largeEffect : smallVar : Clearly large|and positive : different
		PU : largeEffect : largeVar : Clearly positive,|maybe large : different
		PS : smallEffect : smallVar : Clearly positive|and not large : different
		US : tinyEffect : medVar : Maybe positive,|clearly small : different
		UU : smallEffect : largeVar : Not both (large|and negative) : different
		nopower : tinyEffect : hugeVar : Should have|done a power|analysis first : different
	")
)
vf <- (vr
	|> mutate(NULL
		, pic = factor(pic, levels=rev(pic))
		, val = vget(val)
		, unc = vget(unc)
		, lwr = val-unc
		, upr = val+unc
		, atext = gsub('\\|', '\n', atext)
	)
)
print(ggplot(vf)
	+ aes(val, pic)
	+ geom_pointrange(aes(xmin=lwr, xmax=upr))
    + theme(panel.grid.minor.x = element_blank())  ## delete vertical grid lines
	+ geom_vline(xintercept=c(0, -1, 1), lty = c(1, 2, 2))
	+ scale_x_continuous(limits=c(-span, span)
		, breaks = -1:1
		, labels = c("cutoff\n(SESOI)", 0 , "cutoff\n(SESOI)")
	)
	+ scale_y_discrete(labels=rev(vf$atext))
  + labs(x = "", y = "")
)
```

As an example, we'll start by determining the sample size to needed to get 80% power for a cutoff/SESOI value (`s` in the underlying code) with a standard deviation of 1:

```{r pow1}
pp <- power.t.test(delta = 1, sd = 1, power = 0.8)
n <- ceiling(pp$n)  ## 17
tt <- power.t.test(delta = 1, sd = 1, n = n)
pow1 <- round(tt$power,3) ## 0.807
```

With per-sample $n$ of `r n` (and with $\delta=1$, $\sigma=1$, $\alpha=0.05$)  we get a power of `r pow1`.

If we simulate 10,000 such experiments, here is the breakdown of different outcomes:

```{r simfun0}
set.seed(101)
t0 <- system.time(
  tt0 <- tabfun(n=17, nsim =  10000)
)
pow1_sim <- sum(tt0[1:3])
stopifnot(all.equal(pow1_sim, tt$power, tolerance = 2e-3))
print(tt0)
```

We confirm that we do get overall power, i.e. all outcomes with a clear sign, very close to the nominal value (`r round(pow1_sim, 3)`)

### Simulations with $\delta=2/3$, $s=1$, varying $n$

```{r sim1, cache=TRUE}
set.seed(101)
nvec <- c(5:10, (2:9)*10, 100, 200)
mm <- lapply(nvec, tabfun, delta=2/3, nsim = 10000) |> do.call(what = rbind)
```

```{r sim1_sum}
f_widen <- function(x) {
  (x 
    |> as.data.frame()
    |> dplyr::mutate(n = nvec, .before = 1)
    |> tidyr::pivot_longer(col = -n)
    |> dplyr::mutate(name = factor(name, levels = levs))
  )
}
mmw <- f_widen(mm)
```

```{r sim1_plot, fig.width = 8}
gg1 <- ggplot(mmw, aes(n, value, colour = name)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(y = "proportion", x = "sample size per group") +
  out_scale
## see https://tdhock.github.io/directlabels/docs/index.html
##  for direct labeling choices
print(direct.label(gg1, "top.bumptwice"))
```

If the true effect size ($\delta = 1$) is equal to the cutoff/SESOI ($s=1$) then everything converges
to a narrow window around the SESOI: mostly "unclear magnitude/clear sign"
but always a clear sign. Indeed, we can see that for $n=100$ we have converged (for 95% CI/$\alpha = 0.05$) to all "clear sign" (i.e. power $\approx$ 100%) with 95% in the "unclear magnitude" category and 2.5% in "clearly small" or "clearly large".

```{r mm_tail}
drop(tail(mm,1))
```

### Simulations with $\delta=1.5$, $s=1$, varying $n$

What if we make the true effect size greater than the SESOI?

```{r sim2, cache=TRUE}
mm2 <- lapply(nvec, tabfun, nsim = 10000, delta = 1.5) |> do.call(what = rbind)
```
```{r sim2_plot, fig.width = 8}
mmw2 <- f_widen(mm2)
print(direct.label(gg1 %+% mmw2, "top.bumptwice"))
```

## next steps

* Shiny app?
* add 'total power' (i.e. sum of all 'clear sign' outcomes) to plots?
* other examples?
* it would be elegant and computationally efficient to figure out the analytical results for this (i.e., probably in terms of non-central $\chi^2$ distributions - @owenSpecial1965 or @juliousSample2004 seem like useful starting points), although 10,000 simulations take only about `r round(t0[["elapsed"]], 2)` seconds (and could possibly be sped up a bit as well by eliminating overhead)
* change 'sign' to 'positive' in plots? (JD assumes a positive effect so the first plot can be labeled more clearly; maybe this is worth carrying forward)

## references
